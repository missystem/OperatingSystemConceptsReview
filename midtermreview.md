## DEFINITIONS OF COMPUTER SYSTEM COMPONENTS
* **CPU** — The hardware that executes instructions
* **Processor** — A physical chip that contains one or more CPUs
* **Core** — The basic computation unit of the CPU
* **Multicore** — Including multiple computing cores on the same CPU
* **Multiprocessor** — Including multiple processors

---

## [Lecture 01: Introduction to Operating Systems (Chapter 1)](https://github.com/missystem/cis415review/blob/master/lecture-1-introduction.pdf)

### What is Operating System
- a program that acts as an intermediary between a user of a computer and the computer hardware

### What does an OS do?
* Acts as a Resource manager
* Process
	- hides programs from one another
* Traffic cop
	- resource management
	- decide which program to run and when
* Memory management
	- protection from other programs' mistakes
* Security
	- protection from other programs' malice
* System call interface
	- abstract, simplified interface to services
	- like a function library, but communicates with the OS
* Device management
* Communication
	- between processes
	- to devices & networks

 
### OS system goals
* Execute user programs
* Make solving user problems easier
* Make the computer system covenient to use
* Use the computer hardware in an efficient manner

### Computer System Structure
* Hardware
	- provides basic computing resources
	- CPU, memory, I/O devices
* Operating system
	- controls and coordinates use of hardware among various applications and users
* Application programs
	- use system resources to solve computing problems
	- word processors, compilers, web browsers, database systems, video games, ...
* Users
	- People, machines, other computers
 <img src="https://github.com/missystem/cis415review/blob/master/ch1_OSstructure.png">

### Basic Computer System Organization
* Computer-system operation
	- One or more CPUs, device ontrollers connect through common bus providing access to shared memory
	- Concurrent execution of CPUs and devices competing for memory cycles
<img src="https://github.com/missystem/cis415review/blob/master/ch1_systembus.png">

### Computer System Architecture
* Traditionally, most systems use a single general-purpose processor 
	- Most systems have special-purpose processors as well
* Modern computers use multiprocessor systems now
	- Advantages:
		- Increased throughput
		- Economy of scale
		- Increased reliability – graceful degradation or fault tolerance
	- Types:
		- Symmetric multiprocessing (SMP) - each processor performs all tasks
		- asymmetric multiprocessing – each processor has a specific task

	- Can run parallel applications

### Computer System Operation
* I/O devices & the CPU can execute concurrently
* Each device controller 
	- is in charge of a device type
	- has a local buffer
* CPU moves data between main memory & local buffers associated with I/O devices
* I/O is from the device -> local buffer of controller
* Device controller informs CPU that it has finished its operation by causing an **interrupt**

### What Operating Systems Do?
* User view:
	- ease of use, convenience, performance
	- dedicated resources
* Server (System) view:
	- shared resources
	- keep all users happy

### Operating System Definition
* Resource allocation and control
* as Resource allocator
	- manages all resources (CPU time, memory space, storage space, I/O devices, etc)
	- Figure possibly conflicting requests for resources
		- OS decides how to allocate them to specific programs and users so that it can operate the computer system efficiently and fairly.

* as Control program
	- manages the execution of user programs to prevent errors and improper use of the computer
* Different view on scope:
	- it includes everything a vendor ships when you order “the operating system”
	- the OS is the one program running at all times on the computer — usually called the kernel
		- 2 other types of programs: 
			- system programs, which are associated with the operating system but are not necessarily part of the kernel
			- application programs, which include all programs not associated with the operation of the system.

### Page Fault Handling
* is caused when access to a virtual memory location is not found in physical memory
* What happens?
	- Interrupt is generated by the hardware
	- Handler in OS determines how to obtain memory
	- If page is still on disk, then handler (page: fixed sized block of data)
		- allocates physical page
		- makes I/O request to disk via file system and driver
	- Driver copies page from disk into new physical page
	- OS restarts the process at the interrupted instruction
* Multiple processes -> more complex
* OS has to make trade-offs

### Learning about OS
* OS has many protocols like page fault handling
* OS designers add layers of abstraction to simplify programming (e.g., virtual memory)
* The design of protocols using these concepts involves trade-offs (e.g., disk performance)

### Device I/O - How a modern computer works?
How a modern computer system works?
	von Neumann architecture
<img src="https://github.com/missystem/cis415review/blob/master/ch1_von_Neumann_architecture.png">

### Common Functions of Interrupts
* Interrupt transfers control to the interrupt service routine, generally through an interrupt vectors
* The operating system preserves the state of the CPU by storing registers and the program counter
* Determines which type of interrupt has occurred: 
	- Polling
	- Vectored interrupt system
* Interrupt architecture must save the address of the
interrupted instruction
* A trap (or an exception) is a software-generated interrupt, caused 
	- by an error (division by zero or invalid memory access)
	- by a specific request from a user program, that an operating-system service be performed by executing a special operation called a system call
* An operating system is interrupt driven

### I/O Structure
After I/O starts, control returns to user program ...
* Only upon I/O completion
	- wait instruction idles the CPU until the next interrupt 
	- wait loop (contention for memory access)
	- at most one I/O request is outstanding at a time
* Without waiting for I/O completion
	- system call – request to the OS to allow user to wait for I/O completion
	- device-status table contains entry for each I/O device indicating its type, address, and state
	- OS indexes into I/O device table to determine device status and to modify table entry to include interrupt

### Interrupt Timeline
<img src="https://github.com/missystem/cis415review/blob/master/ch1_interrupt_timeline.png">

### I/O Sturcture - 2 Methods
<img src="https://github.com/missystem/cis415review/blob/master/ch1_two_IO_methods.png">
* Synchronous
	- After I/O starts, control returns to user program only upon I/O completion
	- A wait instruction idles the CPU until next interrupt
	- Or the CPU runs a wait loop (problems?)
	- No simultaneous I/O processing (only 1 request)
* Asynchronous
	- After I/O starts, control returns to user program without waiting for I/O completion
	- User program then waits for I/O complete
	- I/O interrupts upon completion

### Device Status Table
<img src="https://github.com/missystem/cis415review/blob/master/ch1_device_status_table.png">
* When a kernel supports asynchronous I/O, it must be able to keep track of many I/O requests at the same time. For this purpose, the operating system might attach the wait queue to a device-status table. The kernel manages this table, which contains an entry for each I/O device. Each table entry indicates the device’s type, address, and state (not functioning, idle, or busy). If the device is busy with a request, the type of request and other parameters will be stored in the table entry for that device.

### Timer Interrupts for Kernel Control
* Prevent infinite loop / process hogging resources
	- Timer is set to interrupt the computer after some time period
	- Keep a counter that is decremented by the physical clock
	- Operating system set the counter (privileged instruction)
	- When counter reaches zero, generate an interrupt
	- Set up before scheduling process to regain control or terminate program that exceeds allotted time
<img src="https://github.com/missystem/cis415review/blob/master/figure1.13.png">

### Process Management
* A **process** is a program in execution
	- A **program** is a passive entity
	- A process is an active entity (unit of work in the system)
* Process needs resources to accomplish its task
	- CPU, memory, I/O, files
	- Initialization data
* Process termination requires reclaim of any reusable resources
* **Single-threaded process** has one program counter specifying location of next instruction to execute
	- Process executes instructions sequentially
	- One at a time, until completion
* **Multi-threaded process** has one program counter per thread
* Typically system has many processes, some user, some operating system running concurrently on one or more CPUs
	- Concurrency by multiplexing the CPUs among the processes / threads

### Process Management Activities
* Creating & deleting both user and system processes
* Suspending & resuming processes
* Providing mechanisms for 
	- process synchronization
	- process communication
	- deadlock handling

### Scheduling
* Determine which task to perform given:
	- Multiple user processes
	- Multiple hardware components
* Provide effective performance
	- Responsive to users, CPU utilization
* Provide fairness
	- Avoid starvation for low priority processes

### Memory Management
* To execute a program, all (or part) of the instructions must be in memory
* All (or part) of the data that is needed by the program must be in memory
* Memory management determines what is in memory and when
	- Optimizing both the CPU utilization and the computer's responding speed users
* Memory management activities
	- keeping track of which parts of memory are currently being used and by whom
	- deciding which processes (or parts thereof) and data to move into and out of memory
	- allocating and deallocating memory space as needed

### Storage Hierarchy
<img src="https://github.com/missystem/cis415review/blob/master/figure1.6.png">

### Storage Management
* OS provides uniform, logical view of information storage
	- Abstracts physical properties to logical storage unit - file
	- Each medium is controlled by device
		- varying properties include: access speed, capacity, data-transfer rate, access method (sequential or random)
* File-System management
	- Files usually organized into directories
	- Access control on most systems determines who can access what
	- OS activities include
		- creating and deleting files and directories
		- primitives to manipulate files and directories
		- mapping files onto secondary storage
		- backup files onto stable (non-volatile) storage media

### Protection and Security
* Protection
	- any mechanism for controlling access of processes or users to resources defined by the OS
* Security
	- defense of the system against internal and external attacks
* Systems generally first distinguish among users, ways to determine who can do what:
	- User identities (user IDs, security IDs) 
		- include name and associated number, one per user
	- User ID
		- associated with all files, processes of that user to determine access control
	- Group identifier (group ID)
		- allows set of users to be defined and controls managed, then also associated with each process, file
	- Privilege escalation 
		- allows user to change to effective ID with more rights

### OS Structures
* Multiprogramming (batch system)
	- Single user cannot keep CPU and I/O devices busy at all times
	- Multiprogramming organizes jobs (code & data) so CPU always has one to execute
	- A subset of total jobs in system is kept in memory
	- One job selected and run via job scheduling
	- When it has to wait (e.g., I/O), OS switches to another job
* Multitasking (timesharing)
	- Logical extension of multiprogramming
	- The CPU executes multiple processes by switching among them frequently
	- Response time should be < 1s
	- Each user has ≥ 1 program executing in memory
	- If several jobs ready to run at the same time, system chooses which process will run next (CPU scheduling)
	- Virtual memory allows the execution of a processes that is not completely in memory (ensure reasonable response time)

### Computing Environments – Traditional
* Stand-alone general purpose machines
* But blurred as most systems interconnect with others (i.e., the Internet)
* Portals provide web access to internal systems
* Network computers (thin clients) are like Web terminals
* Mobile computers (Smart phones) interconnect via wireless networks
* Networking becoming ubiquitous 
	- even home systems use firewalls to protect home computers from Internet attacks

### Computing Environments – Distributed
* Collection of separate, possibly heterogeneous, systems networked together
	- network is a communications path, TCP/IP most common
		- Local Area Network (LAN)
		- Wide Area Network (WAN)
		- Metropolitan Area Network (MAN)
		- Personal Area Network (PAN)
* Network OS provides features between systems across network
	- communication scheme allows systems to exchange messages
	- illusion of a single system

### Computing Environments – Client-Server
* Dumb terminals supplanted by smart PCs
* Many systems now servers, responding to requests generated by clients
	- compute-server system
		- provides an interface to client to request services (i.e., database)
	- file-server system
		- provides interface for clients to store and retrieve files
<img src="https://github.com/missystem/cis415review/blob/master/figure1.22.png">

### Computing Environments – Peer-to-Peer (P2P)
* Another model of distributed system
* Does not distinguish clients & servers
	- all nodes are considered peers
	- each node may act as client, server or both
	- node must join P2P network
		- registers its service with central lookup service on network, or
		- broadcast request for service and respond to requests for service via discovery protocol
	- Examples: Napster and Gnutella, Voice over IP (VoIP) (e.g., Skype)

### Computing Environments – Mobile
* Handheld smartphones, tablets, ...
* What is the functional difference between them and a "traditional" laptop?
	- Extra feature 
	- more OS features (GPS, gyroscope)
	- Allows new types of apps like augmented reality
	- Use IEEE 802.11 wireless, or cellular data networks for connectivity
	- Leaders: Apple IOS, Google Android
	- "Internet of Things" (IoT)

### Computing Environments – Virtualization
* Allows OSes to run applications within other OSes
* Emulation used when source CPU type different from target type (i.e., PowerPC to Intel x86)
	- Generally slowest method
	- When computer language not compiled to native code - Interpretation
* Virtalization
	- OS natively compiled for CPU, running guest OSes also natively compiled
		- Consider VMware running WinXP guests, each running applications, all on native WinXP host OS
		- VMM (Virtual Machine Manager) provides virtualization services
* Use cases involve laptops and desktops running multiple OSes for exploration or compatibility
	- Apple laptop running Mac OS X host, Windows as a guest
	- Developing apps for multiple OSes without having multiple systems
	- QA testing applications without having multiple systems
	- Executing and managing computing environments within data centers
* VMM can run natively, in which case they are also the host
	- Then there is no general purpose host (VMware ESX and Citrix XenServer)

### Computing Environments – Cloud Computing
* Delivers computing, storage, even applications as a service across a network
	- IaaS, PaaS, SaaS
* Logical extension of virtualization because it uses virtualization as the base for it functionality
* Cloud computing environments composed of traditional OSes, plus VMMs, plus cloud management tools
	- Internet connectivity requires security like firewalls
	- Load balancers spread traffic across multiple applications
<img src="https://github.com/missystem/cis415review/blob/master/figure1.24.png">

### Open Source Operating Systems
* Operating systems made available in source-code format rather than just binary closed-source
* Counter to the copy protection and Digital Rights Management (DRM) movement
* Started by Free Software Foundation (FSF), which has “copyleft” GNU Public License (GPL)
* Examples include GNU/Linux and BSD UNIX (including core of Mac OS X), and many more
* Can use VMM like VMware Player (Free on Windows), VirtualBox (open source and free)
	- Use to run guest operating systems for exploration

---

## [Lecutre 02: OS Structure and System Calls (Chapter 2)](https://github.com/missystem/cis415review/blob/master/lecture-2-structure.pdf)

## Outline
* Hardware and OS relationship
* [Operating System Services](https://github.com/missystem/cis415review/blob/master/lecturenotes02.md#operating-system-services)
* [System Calls](https://github.com/missystem/cis415review/blob/master/lecturenotes02.md#systems-calls)
* [Types of System Calls](https://github.com/missystem/cis415review/blob/master/lecturenotes02.md#types-of-system-calls)
* [System Programs](https://github.com/missystem/cis415review/blob/master/lecturenotes02.md#system-programs-system-services)
* [Operating System Design and Implementation](https://github.com/missystem/cis415review/blob/master/lecturenotes02.md#os-design-and-implementation)
* [Operating System Structure](https://github.com/missystem/cis415review/blob/master/lecturenotes02.md#operating-system-structure)
* [Building and Booting an Operating System](https://github.com/missystem/cis415review/blob/master/lecturenotes02.md#building-and-booting-an-operating-system)
* [Operating System Debugging](https://github.com/missystem/cis415review/blob/master/lecturenotes02.md#operating-system-debugging)
* [Summary](https://github.com/missystem/cis415review/blob/master/lecturenotes02.md#summary)


### Objectives
* To describe computer system organization
* To describe the services an operating system provides to users, processes, and other systems
* To discuss the various ways of structuring an operating system
* To explain how operating systems are installed and customized and how they boot

### Canonical System Hardware
* CPU
	- processor to perform computations
* Memory
	- hold instructions and data
* I/O devices
	- disk, monitor, network, printer
* Bus
	- systems interconnection for communication
<img src="https://github.com/missystem/cis415review/blob/master/canonical_system_hardware.png">

### CPU Architecture
* CPUs are semiconductor device with digital logic
	- Arithmetic Logical Unit (ALU)
	- Program Counter (PC) and registers
	- Instruction Set Architecture (ISA)
* Registers
	- part of ISA
	- CPU's scratchpads for program execution
	- fastest memory available in a computer system
	- instruction, data, address (n-bit architecture, 32-bit or 64-bit)
* Cache
	- fast memory (close to CPU)
	- faster than main memory, but more expensive
	- managed by the hardware (not seen by the OS)
* Clock to synchronizes constituent circuits

### Memory
* Random Access Memory (RAM)
	- Semiconductor DIMMs on PCB
	- Volatile Dynamic RAM (DRAM)
* Use as "main" memory for instruction and data
* OS manages main memory
	- It is a resource necessary for any code execution
* CPU fetches instruction and data from memory
	- into its instruction processing
	- into its cache / registers
* Memory controller implements logic for:
	- reading / writing to DRAM
	- refreshing DRAM to maintain contents
	- address translation circuitry for virtual memory

<img src="https://github.com/missystem/cis415review/blob/master/pc_motherboard.png">

### I/O Devices, Hard Disks, and SSD
*  Large variety, varying speeds, lot of diversity
	- Disk, tape, monitor, mouse, keyboard, NIC, ...
	- Serial or parallel interfaces
* Each device has a **controller**
	- Hides low-level details from OS (hardware interface)
	- Manages data flow between device and CPU/memory
* Hard disks are *secondary storage devices*
	- Mechanically operated with sequential access
	- Cheap (bytes / $), but slow
	- Orders of magnitude slower than main memory
* Solid state devices (SSD) are increasingly common

### Interconnects
* **bus** - hardware interconnect for supporting the exchange of data, control, signals, ...
	- Physicalspecification
	- Defined by a protocol
	- Data and control arbitration
* System bus - for CPU connection to memory primarily
	- Main bus going to CPU, Memory, data
	- Also to bridge to device buses
* PCI bus - for devices
	- Connects CPU-memory subsystem to:
		- fast devices
		- expansion bus that connects slow devices
* Other device "bus" types
	- SCSI, IDE, USB, ...
<br /> PCI adaptor -> I/O controller
<br /> yellow part -> I/O bus <br />
<img src="https://github.com/missystem/cis415review/blob/master/system_bus.png">

### Operating System Services
* Provides an environment for program execution
* For helping the user
	- User Interface (UI)
		- A window system with a mouse that serves as a pointing device to direct I/O, choose from menus, and make selections and a keyboard to enter text
		- Commonly, graphical user interface (GUI) is used
		- Mobile systems such as phones and tablets provide a touch-screen interface
		- command-line interface (CLI)
			- uses text commands and a method for entering them
	- Program execution (load, run, terminate)
	- I/O operations (file or device)
	- File-system manipulation (file, directories, permission)
	- Communications (same computer or several over network) 
	- Error detection (hardware and software)
* For ensuring the efficient operation of the system
	- Resource allocation (across multiple, concurrent jobs)
	- Logging (or Accounting) (how much and what kind of resources)
	- Protection and security (resources, users)
![Figure 2.1 A view of operating system services](https://github.com/missystem/cis415review/blob/master/figure2.1_view_of_OS_services.png)

### OS Services and Hardware Support
* Protection
	- Kernel / User mode and protected instructions
	- Base / Limit registers
* Scheduling
	- Timer
* System Calls
	- Trap instructions
* Efficient I/O
	- Interrupts
	- Memory-mapping
* Synchronization
	- Stomic Instructions
* Virtual memory
	- Translation Lookaside Buffer (TLB)

### Kernel / User Mode
* A modern CPU has at least two execution modes
	- prevent user programs from interfering with the proper operation of the system
	- indicated by status bit in a protected CPU register
	- OS kernel runs in privileged mode (also called *kernel mode* or *supervisor mode*)
	- Applications run in normal mode (i.e., not privileged)
* OS can switch the processor to user mode
	- CPU then can only access user process address space
	- Can not do other things as well, such as talk to devices
* Certain events need the OS to run
	- must switch the processor to privileged mode
	- e.g., I/O control, timer management, and interrupt management
* OS redefinition
	- Software that runs in privileged mode

### Protected Instructions
* Instructions that require privilege, such as:
	- Direct access to I/O
	- Modify page table pointers or the TLB
		- page: fixed sized block of data
	- Enable and disable interrupts
	- Halt the machine
* Access sensitive registers or perform sensitive
operations
* Only allow access in privilege modes
	- Otherwise, random programs may crash the machine

### Base and Limit Registers
* User processes must be protected from each other
* OS must be protected from user processes
* Memory referencing hardware to protect memory regions
	- Base register contains an address offset in physical memory (holds the smallest legal physical memory address)
	- Limit register is the maximum address that can be referenced (specifies the size of the range)
	- Both can be loaded only by the operating system before execution
* CPU checks each memory reference to make sure it is in proper range
* Both for instruction and data addresses
* Ensures references can not access other memory regions and corrupt memory

### Interrupts
* A key way in which hardware interacts with the operating system
* A hardware device triggers an interrupt by sending a signal to the CPU to alert the CPU that some event requires attention
* The interrupt is managed by the interrupt handler
* Interrupts make events that occur in the system visible for the OS needs to observe
* Interrupts can occur at any time
* OS polls for events (polling)
	- Inefficient use of resources
* OS is interrupted when events occur
	- I/O device has own logic (processor)
	- When device operation finishes, it pulls on interrupt bus
	- CPU “handles” interrupt

### Interrupt Vectoring
* Interrupts are asynchronous signals that indicate some need for attention by the OS
	- Replaces polling for events
* Represent:
	- Normal events to be noticed and acted upon
		- device notification
		- software system call
	- Abnormal conditions to be corrected (divide by 0)
	- Abnormal conditions that cannot be corrected (disk failure)
* Interrupt vectors are used to decide what to do with different interrupts
	- Address where interrupt routines live in the OS for different interrupt types
 <img src="https://github.com/missystem/cis415review/blob/master/figure1.5_Intel_processor_event_vector_table.png">

### Hardware Interrupts
* Signal from a device
	- Implemented by a controller (e.g., memory)
* Examples
	- Timer (clock)
	- keyboard, mouse
	- end of DMA transfer
* Response to processor request
* Unsolicited response
	- asynchronous
<img src="https://github.com/missystem/cis415review/blob/master/logical_diagram_of_interrupt_routing.png">

### Timer 
* OS needs timers for:
	- Time of day, CPU scheduling, ...
	- There can be multiple timers for different things
* Based on a hardware clock source (e.g., 1 Ghz)
	- Count-down timer (e.g., 1 Ghz to 1 Khz)
	- Generates an interrupt at 0 (e.g., every 1 msec)
* Use timers to ensure that certain future events occur
* Most importantly, it is a way for OS to regain control
* User programs can set up their own “software” timers

### Software Interrupts
* Software interrupts (traps)
	- Special interrupt instructions
		- int is the interrupt instruction
		- int 0x80 passes control to interrupt vector 0x80
	- Exceptions
		- can be fixed (e.g., page fault)
		- cannot be fixed (e.g., divide by zero)
* All invoke OS, just like a hardware interrupt
	- Trap starts running OS code in supervisor access space
	- This space can not be overwritten by the user program

### How Does a Process Run? (high level view)
* OS keeps track of which process is assigned to which sections in memory along with other details
* For a new process to run, memory is assigned by the OS, which puts the code in that location
	- switch to user mode
	- start running at first address of the program
* OS keeps record of every process
	- This is the *process context*
	- Assigned memory, current Program Counter, ...
	- Enough info to restart process where it left off

### Then along comes an interrupt ...
1. A hardware interrupt or a trap (software interrupt) happens
	- Example: received input from keyboard
2. OS records state of running process’s context
	- stored in a Process Control Block (PCB)
3. OS services the interrupt
	- Example: send something to the printer
4. OS picks process to restart
	- which process is picked depends on scheduling
	- moves back into user mode

### Interrupt Handling
* Each interrupt has an *interrupt handler*
* When an interrupt request (IRQ) is received
	- If interrupt mask allows interrupt, then ...
		1. save state of current process
			- at time of interrupt something else may be running
			- state: Registers (stack pointer), program counter, ...
		2. execute handler
		3. return to current process or another process

Interrupt (Trap) Handlers
<img src="https://github.com/missystem/cis415review/blob/master/interrupt_handler.png">

Multiple Interrupts
<img src="https://github.com/missystem/cis415review/blob/master/multiple_interrupts.png">

### Device Access
* Port I/O
	- uses special I/O instructions
	- uort number, device address (not process address)
* Memory-mapped I/O
	- uses memory instructions (load/store)
		- memory-mapped device registers
	- does not require special instructions <br />
[<img src="https://github.com/missystem/cis415review/blob/master/isolated_vs_memory_mapped_I:O.png">](http://ece-research.unm.edu/jimp/310/slides/8086_IO1.html)
IORC: I/O Read Control <br />
IOWC: I/O Write Control

### Direct Memory Access (DMA)
* Direct access to I/O controller through memory
* Reserve area of memory for communication with
the I/O device
	- Video RAM
		- CPU writes frame buffer
		- video card displays it
	- Network interfaces
* DMA is efficient and convenient and fast
	- Multiple components can talk to other components concurrently, rather than competing for cycles on a shared bus
	- After setting up buffers, pointers, and counters for the I/O device, the device controller transfers an entire block of data directly to or from the device and main memory, with no intervention by the CPU
	- Only one interrupt is generated per block, to tell the device driver that the operation has completed, rather than the one interrupt per byte generated for low-speed devices
	- While the device controller is performing these operations, the CPU is available to accomplish other work
<img src="https://github.com/missystem/cis415review/blob/master/figure1.7_how_modern_computers_work.png">

### Synchronization
* How can OS synchronize concurrent processes?
	- multiple threads, processes, interrupts, DMA
* CPU must provide mechanism for atomicity
	- Series of instructions that execute as one or not at all
* One approach:
	- Disable interrupts, perform action, enable interrupts
	- Advantages:
		- Requires no hardware support
		- Conceptually simple
	- Disadvantages:
		- Could cause starvation
* A Modern Synchronization Approach
	- Use hardware support for atomic instructions
		- Small set of instructions that cannot be interrupted
	- Examples:
		- **Test-and-set (TST)** <br />
		if word contains given value, set to new value
		- **Compare-and-swap (CAS)** <br />
		if word equals value, swap old value with new
		- Intel: LOCK prefix (XCHG, ADD, DEC, ...)
	- Used to implement locks 

### Process Address Space
* Process Address Space is all locations that are addressable by the process
* Every running program can have its own private address space
* Can restrict use of addresses so as to isolate different area
	- Restrictions are enforced by OS
	- Text section
		- read only program instructions are stored in
		- the executable code
	- Data section
		- hold the data for the running process (read/write)
		- global variables
	- Heap section
		- memory that is dynamically allocated during program run time
		- allows for dynamic data expansion
	- Stack section
		- temporary data storage when invoking functions <br />
		(such as function parameters, return addresses, and local variables)
	- the sizes of the text and data sections are fixed, as their sizes do not change during program run time <br />
	the stack and heap sections can shrink and grow dynamically during program execution
<img src="https://github.com/missystem/cis415review/blob/master/figure3.1_layout_of_a_process_in_memory.png">

### Virtual Memory
* Provide the illusion of infinite memory
* OS loads pages from disk as needed
	- page: fixed sized block of data
* Benefits
	- Allows the execution of programs that may not fit entirely in memory
* OS needs to maintain mapping between physical and virtual memory
	- Page tables stored in memory

### Address Translation Hardware
* Early virtual memory systems used to do translation in software
	- meaning the OS did it
	- an additional memory access for each memory access
* Address translation hardware solved this problem
	- Translation Look-aside Buffer (TLB)
* Modern CPUs contain TLB hardware
	- Fmall, fast-lookup hardware cache
	- Modern workloads are TLB-miss dominated (page number is not in the TLB)

#### Takeaway
* Modern architectures provide lots of features to help the OS do its job
	- Protection mechanisms (modes)
	- Interrupts
	- Device I/O
	- Synchronization
	- Virtual Memory (TLB)
* Otherwise impossible or impractical in software

### Operating System Layers
* Application
* Libraries (in application process)
* System Services
* OS API
* OS Kernel
* Hardware
<img src="https://github.com/missystem/cis415review/blob/master/unix_architecture.png"> 

### Applications to Libraries
* Application Programming Interface (API)
* Libraries (e.g., *libc*)
* Library routines (e.g., *printf()* of stdio.h)
* All within the process's address space
	- Statically linked
		- libraries are included as part of the application code
		- calls are resolved at compile time
	- Dynamically linked
		- libraries are loaded by the OS at execution time as needed
		- jump tables and pointers are resolved dynamically by linker

### Application to (System) Services
* Provide syntactic sugar for using resources
	- Printing
	- Program management
	- Network management
	- File management
* Provide special functions beyond OS
* [UNIX man pages](http://man7.org/linux/man-pages/man7/man-pages.7.html), sections 1 and 8
* Command line system programs

### Libraries to System Routines
* System call interface
	- [UNIX man pages](http://man7.org/linux/man-pages/man7/man-pages.7.html), sections 2
	- Examples:
		- open(), [read()](http://man7.org/linux/man-pages/man2/read.2.html), [write()](http://man7.org/linux/man-pages/man2/write.2.html) – defined in unistd.h
	- Call these via libraries? 
		- [fopen()](http://man7.org/linux/man-pages/man3/fopen.3.html) vs. [open()](http://man7.org/linux/man-pages/man2/open.2.html)
* Special files
	- Drives
	- [*/proc*](http://man7.org/linux/man-pages/man5/proc.5.html)
	- [*sysfs*](http://man7.org/linux/man-pages/man5/sysfs.5.html)

### System to Hardware
* Software-Hardware interface
* OS Kernel functions
	- Concepts		<=> Managers (hardware)
	- Files 		<=> File systems (drivers and devices)
	- Address space <=> Virtual memory (memory)
	- Programs 		<=> Process model (CPU, ISA)
* OS provides abstractions of devices and hardware objects
	- These abstractions are represented in software running in the OS and data structures that it maintains

### Systems Calls
* Programming interface to OS services via system libraries
* Typically written in a high-level language (C, C++) 
* Mostly accessed by programs via a high-level Application Programming Interface (API) (versus direct system call use)
	- Win32 (Windows), POSIX (Unix, Linux, MacOS), Java (JVM)
* Typically, a number is associated with each system call
	- System-call interface maintains a table indexed by call #
* System call interface invokes the intended system call in OS kernel and returns status of the system call and return values
* Caller just obeys API and understand what OS will do
	- Most details of OS interface hidden by API

### Types of System Calls
* **Process control**
	- create process, terminate process
	- load, execute
	- get process attributes, set process attributes
	- wait event, signal event
	- allocate and free memory
* **File management**
	- create file, delete file
	- open, close
	- read, write, reposition
	- get file attributes, set file attributes
* **Device management**
	- request device, release device
	- read, write, reposition
	- get device attributes, set device attributes 
	- logically attach or detach devices
* **Information maintenance**
	- get time or date, set time or date
	- get system data, set system data
	- get process, file, or device attributes
	- set process, file, or device attributes
* **Communications**
	- create, delete communication connection
	- send, receive messages
	- transfer status information
	- attach or detach remote devices
* **Protection**
	- get file permissions
	- set file permissions

#### The handling of a user application invoking the open() system call
<img src="https://github.com/missystem/cis415review/blob/master/figure2.6_handling_of_user application_invoking_open().png"> 

#### Standard C Library Example (printf())
<img src="https://github.com/missystem/cis415review/blob/master/printf().png"> 

#### System Call Example getpid()
<img src="https://github.com/missystem/cis415review/blob/master/getpid().png"> 

### System Call Handling
<img src="https://github.com/missystem/cis415review/blob/master/system_call_handling.png"> 

### System Call Process
1. Procedure call in user process
2. Initial work in user mode
3. Trap instruction to invoke kernel
4. Preparation
5. I/O command
6. Wait
7. Completion interrupt handling
8. Return-from-interrupt instruction
9. Final work in user mode
10. Ordinary return to user code

<ins>system operations</ins><br />
libc <br />
int 0x80 <br />
sys_read, mmap2 <br />
read from disk <br />
disk is slow <br /><br /><br />
libc

### Details on x86 / Linux
* A more accurate picture:
	- Consider a typical Linux process
	- Its “thread of execution” can be several places
		- in your program’s code
		- in glibc, a shared library containing the C standard library, POSIX support, and more
		- in the Linux architecture - independent code
		- in Linux x86-32/x86-64 code <br />
<img src="https://github.com/missystem/cis415review/blob/master/system_execution_detail_01.png"> <br />

* Some routines your program invokes may be **entirely handled by glibc**
	- Without involving the kernel
		- e.g., strcmp( ) from stdio.h
	- some initial overhead when invoking functions in dynamically linked libraries ...
	- ... but after symbols are resolved, invoking glibc routines is nearly as fast as a function call within your program itself <br />
<img src="https://github.com/missystem/cis415review/blob/master/system_execution_detail_02.png"> <br />

* Some routines may be **handled by glibc, but they in turn invoke Linux system calls**
	- Example: POSIX wrappers around Linux syscalls
		- POSIX *readdir()* --> invokes the underlying Linux *readdir()*
	- Example: C stdio functions that read and write from files
		- *fopen(), fclose(), fprintf()*, ... --> invoke underlying Linux *open(), read(), write(), close()*, ... <br />
<img src="https://github.com/missystem/cis415review/blob/master/system_execution_detail_03.png"> <br />

* Your program can choose to **directly invoke Linux system calls** as well
	- Nothing forces you to link with glibc and use it
	- But relying on directly invoked Linux system calls may make your program less portable across UNIX varieties <br />
 <img src="https://github.com/missystem/cis415review/blob/master/system_execution_detail_04.png"> <br />

### System Programs (System Services)
* System programs provide a convenient environment for program development and execution
* They can be divided into categories: 
	<!-- p75 on the book-->
	- File manipulation (management)
	- Status information
	- File modification
	- Programing-language support (program development)
	- Program loading and execution
	<!-- - Application programs -->
	- Communications
	- Background services
* Most user’s view of the operating system is defined by system programs, not the actual system calls

### File Interface
* Goal:
	- Provide a uniform abstraction for accessing the OS and its resources
* Abstraction
	- File
* Use file system calls to access OS services
	- Devices, sockets, pipes, etc
	- Also use in OS in general

### I/O with System Calls
* Much I/O is based on a streaming model (sequence of bytes)
	- *write()* sends a stream of bytes somewhere
	- *read()* blocks until a stream of input is ready
* Annoying details:
	- Might fail, can block for a while
	- Working with file descriptors
	- Arguments are pointers to character buffers
	- See [read()](http://man7.org/linux/man-pages/man2/read.2.html) and [write()](http://man7.org/linux/man-pages/man2/write.2.html)

### File Descriptors (int fd)
* A process might have several different I/O streams in use at any given time
	- These are specified by a *file descriptor* (a kernel data structure)
		- Each process has its own table of file descriptors
	- [*open()*](http://man7.org/linux/man-pages/man2/open.2.html) **associates** a file descriptor with a file
	- [*close()*](http://man7.org/linux/man-pages/man2/close.2.html) **destroys** a file descriptor
* stdin and stdout are usually associated with a terminal

### Regular File
* File has a *pathname* (e.g.: /tmp/foo)
* Can open the file
	- *int fd = open( “/tmp/foo”, O_RDWR )*
		- *O_RDWR* - flags for read/write access
	- for reading and writing
* Can read from and write to the file
	- *bytes = read( fd, buf, max ); /\* buf get output \*/*
	- *bytes = write( fd, buf, len ); /\* buf has input \*/*
		- *buf* - pointer to buffer

### Socket File
* File has a *pathname* (e.g.: /tmp/bar)
	- Files provide a persistence for a communication channel
	- Usually used for local communication (UNIX domain sockets)
* Open, Read, and Write via socket operations
	- *sockfd = socket( AF_UNIX, TCP_STREAM, 0 );*
	- *local.path* is set to */tmp/bar*
	- *bind ( sockfd, &local, len )*
	- Use sock operations to read and write

### Device File
* Files for interacting with physical devices
	- */dev/null* (do nothing)
	- */dev/cdrom* (CD-drive)
* Use file system operations, but are handled in device-specific ways
	- *open(), read(), write()* correspond to device-specific functions (act as function pointers!)
	- Also, use *ioctl* (I/O control) to interact

### *sysfs* File and */proc* Files
* These files enable reading from and writing to kernel
* [*/proc*](http://man7.org/linux/man-pages/man5/proc.5.html) files
	- Enable reading of kernel state for a process
	- Process information pseudo-file system
	- Does not contain "real" files, but runtime system information
		- system memory
		- devices mounted
		- hardware configuration
	- A lot of system utilities are simply calls to files in this directory
	- By altering files located here you can even read/change kernel parameters (sysctl) while the system is running
* [*sysfs*](http://man7.org/linux/man-pages/man5/sysfs.5.html) files
	- Provide functions that update kernel data
		- file's write function updates kernel based on input data

### Other System Calls
* Hook the output of one program into the input of another
	- *pipe()*
* Block until one of several file descriptor streams is ready
	- *select()*
* Special calls for dealing with network
	- AF_INET sockets, etc
* Send a message to other (or all) processes
	- *signal()*
* Most of these in [section 2](http://man7.org/linux/man-pages/dir_section_2.html) of Unix/Linux manual

### Syscall Functionality
* System calls are the main interface between processes and the OS
	- Like an extended “instruction set” for user programs that hide many details
	- First Unix system had a couple dozen system calls
	- Current systems have many more <br />
		(>300 in Linux and >500 in FreeBSD)

### OS Design and Implementation
* Design and implementation of OS is not “solvable”
	- There are different types for meeting different objectives
	- Some approaches have proven successful
* Internal structure of different OSes can vary widely
* Start the design by defining goals and specifications
* The design of the system will be affected by the choice of hardware and the type of system
* The requirements can be divided into two groups:
	- **user goals**
		- OS should be convenient to use, easy to learn, reliable, safe, and fast
	- **system goals**
		- OS should be easy to design, implement, and maintain, as well as flexible, reliable, error-free, and efficient

### OS Policy vs Mechanism
* Important principle to separate
	- Policy: 	 What will (should) be done?
		- e.g., deciding how long the timer is to be set for a particular user is a policy decision
	- Mechanism: How to do it?
		- e.g., the timer construct is a mechanism for ensuring CPU protection
* The separation of policy and mechanism is important
	- Allows maximum *flexibility* if policy decisions are to be changed later
	- Universal principle
* Specifying and designing an OS is a highly creative task of software engineering
* Policy decisions are important for all resource allocation. 
	- Whenever it is necessary to decide whether or not to allocate a resource, a policy decision must be made. 
	- Whenever the question is how rather than what, it is a mechanism that must be determined.

### Operating System Structure
* Common approach
	- partition the task into small components, or modules
* Monolithic Structure (approach)
	- a common technique for designing operating systems
	- no structure at all
		- place all of the functionality of the kernel into a single, static binary file that runs in a single address space 
	- Tightly coupled system
		- changes to one part of the system can have wide-ranging effects on other parts <br />
<img src="https://github.com/missystem/cis415review/blob/master/figure_2.12_traditional_UNIX_system_structure.png"> <br />
* Layered Approach 
	- Loosely coupled system
		- divided into separate, smaller components that have specific and limited functionality
	- Advantage
		- changes in one component affect only that component, and no others
		- allowing system implementers more freedom in creating and changing the inner workings of the system 
		- avoiding the problems of layer definition and interaction
	- Each layer is implemented only with operations provided by lower-level layers <br />
<img src="https://github.com/missystem/cis415review/blob/master/figure2.14_a_layered_OS.png"><br />
* Microkernels
	- removing all nonessential components from the kernel and implementing them as user-level programs that reside in separate address spaces
	- smaller kernel 
	- minimal process and memory management 
	- Benefits:
		- makes extending the operating system easier
		- easier to port from one hardware design to another
		- more security and reliability 
	- Disadvantages:
		-  the performance of microkernels can suffer due to increased system-function overhead
		- the OS may have to switch from one process to the next to exchange the messages <br />
<img src="https://github.com/missystem/cis415review/blob/master/architecture_of_typical_microkernel.png"><br />
* **Modules**
* Hybrid Systems

### Building and Booting an Operating System
* Operating-System Generation
	1. Write the operating system source code (or obtain previously written source code).
	2. Configure the operating system for the system on which it will run.
	3. Compile the operating system.
	4. Install the operating system.
	5. Boot the computer and its new operating system.
* System Boot
	1. A small piece of code known as the bootstrap program or boot loader locates the kernel.
	2. The kernel is loaded into memory and started.
	3. The kernel initializes hardware.
	4. The root file system is mounted.

### Operating-System Debugging
* Failure Analysis
* Performance Monitoring and Tuning
	- Counters
* Tracing
* BCC (BPF Compiler Collection)
	- a rich toolkit that provides tracing features for Linux systems


### Summary
* Operating systems must balance many needs
	- Job perspective: 
		- give the impression that each process has individual use of system
	- System perspective:
		- Comprehensive management of computer machine resources
* Operating system structures try to make use of system resources straightforward
	- Libraries
	- System services
	- System calls and other interfaces

* An operating system provides an environment for the execution of programs by providing services to users and programs.
* 3 primary approaches for interacting with an operating system
	1. command interpreters
	2. graphical user interfaces
	3. touch- screen interfaces.
* Systemcalls provide an interface to the services made available by an operating system. Programmers use a system call’s application programming interface (API) for accessing system-call services.
* System calls: 6 major categories:
	1. process control
	2. file management
	3. device management
	4. information maintenance
	5. communications
	6. protection.
* The standard C library provides the system-call interface for UNIX and Linux systems.
* OS include a collection of system programs that provide utilities to users
* Linker
	- combines several relocatable object modules into a single binary executable file. 
* Loader 
	- loads the executable file into memory, where it becomes eligible to run on an available CPU.
* Several reasons why applications are operating-system specific.
		- different binary formats for program executables
		- different instruction sets for different CPUs
		- system calls that vary from one operating system to another
* An operating system is designed with specific goals in mind 
	- These goals ultimately determine the operating system’s policies. 
	- An operating system implements these policies through specific mechanisms.
* Monolithic OS 
	- has no structure
	- all functionality is provided in a single, static binary file that runs in a single address space. 
	- disadvantage: difficult to modify
	- primary benefit: efficiency
* Layered OS
	- divided into a number of discrete layers, 
	- bottom layer - hardware interface 
	- highest layer - user interface
	- layered software systems have had some success, 
	- generally not ideal for designing operating systems
		- performance problems
* Microkernel approach
	- uses a minimal kernel
	- most services run as user-level applications
	- Communication takes place via message passing
* Modular approach
	- provides OS services through modules that can be loaded and removed during run time
* Hybrid
	- Many contemporary operating systems are constructed as hybrid systems using a combination of a monolithic kernel and modules.
* Boot loader
	- loads an OS into memory
	- performs initialization
	- begins system execution
* The performance of an OS can be monitored using either counters or tracing. 
	- Counters 
		- a collection of system-wide or per-process statistics
	- Tracing 
		- follows the execution of a program through the operating system

---

## [Lecture 03: Processes (Chapter 3)](https://github.com/missystem/cis415review/blob/master/lecture-3-processes.pdf)

## Outline
* [Process Concept](https://github.com/missystem/cis415review/blob/master/lecturenotes03.md#process-concept)
* [Process Operation](https://github.com/missystem/cis415review/blob/master/lecturenotes03.md#process-creation)
* [System Calls to Create Processes](https://github.com/missystem/cis415review/blob/master/lecturenotes03.md#program-creation-system-calls)
* [Process Scheduling](https://github.com/missystem/cis415review/blob/master/lecturenotes03.md#process-scheduling)
* [Summary](https://github.com/missystem/cis415review/blob/master/lecturenotes03.md#summary)

### Overview of Processes
* How are processes created?
	- from binary program to executing process
* How is a process represented and managed?
	- process creation, process control block
* How does the OS manage multiple processes?
	- process state, ownership, scheduling
* How can processes communicate?
	- interprocess communication, concurrency, deadlock

### Supervisor and User Modes
* OS runs in “supervisor” mode
	- has access to protected (privileged) instructions only
available in that mode (kernel executes in ring 0)
	- allows it to manage the entire system
* OS “loads” programs into processes
	- run in user mode
	- many processes can run in user mode at the same time

### Process Concept
* An operating system executes a variety of programs:
	- Batch system 
		- jobs
	- Time-shared systems
		- user programs or tasks
	- job & process used almost interchangeably
* A process is a **program in execution**
	- Process execution can result in more processes being created
* Multiple parts of a process
	- Program code 
		- image, text
	- Execution state
		- program counter, processor registers, ...
	- Stack 
		- containing temporary data (e.g., call stack frames) 
			- function parameters, return addresses, local variables, ...
	- Data section
		- containing global variables
	- Heap
		- containing memory dynamically allocated during run time

### A Process in Memory
* A process has to reference memory for different purposes
	- Instructions
	- Stack
		- subroutine “frames”, local variables
	- Data
		- static and dynamic (heap)
* Logical memory
	- is what can be referenced by an address
	- \# address bits in instruction address determine logical memory size
	- a “logical address” is from 0 to the size of logical memory (max)
* Compiler and OS determine where things get placed in logical memory
<img width="290" height="474" src="https://github.com/missystem/cis415review/blob/master/process_in_memory.png">

### Process Address Space
* All locations addressable by process
	- Also called logical address space
	- Every process has one
* Restrict addresses to different areas
	- Restrictions enforced by OS
	- Text segment is where read only program instructions are stored
	- Data segment hold the data for the running process (read/write)
		- heap allows for dynamic data expansion
	- Stack segment is where the stack lives
* Process (logical) address space starts at 0 and runs to a high address
<img width="370" height="450" src="https://github.com/missystem/cis415review/blob/master/address_space.png">

### Process Address Space
* Program (Text)
* Global Data (Data)
* Dynamic Data (Heap)
	- grows up
* Thread-local Data (Stack)
	- grows down
* Each thread has its own stack
* \# address bits determine the addressing range <br />
<img width="363" height="493" src="https://github.com/missystem/cis415review/blob/master/process_add_space01.png">

```
int value = 5;					Global

int main() {
	int *p;					Stack

	p = (int *)malloc(sizeof(int));		Heap

	if (p == 0) {
		printf("ERROR: Out of memory\n”);
		return 1; 
	}

	*p = value; 
	printf("%d\n", *p); 

	free(p);
	return 0;
}
```

### [Process Address Space in Action](https://github.com/missystem/cis415review/blob/master/ProcessAddressSpaceInAction.md)

### Process Creation
* What happens?
	- New process object in the OS kernel is created
		- build process data structures
	- Allocate address space (abstract resource)
		- later, allocate actual memory (physical resource)
	- Add to execution (ready) queue
		- make it runnable
* Is the OS a process?
	- Yes, it is the first process when system is booted

### Process Creation Options (Parent and Child)
* Process hierarchy options
	- Parent process (very first process) creates Children processes
	- Child processes can create other child processes
	- Tree of processes
	<img src="https://github.com/missystem/cis415review/blob/master/process_tree.png">
* Resource sharing options
	- Parent and children share all resources
	- Children share subset of parent’s resources
	- Parent and child share no resources
* Execution options
	- Parent and children execute concurrently
	- Parent waits until children terminate
* Address space
	- Child duplicate of parent
	- Child has a program loaded into it

### Executing a Process
* What is required?
* Registers store state of execution in CPU
	- Stack pointer
	- Data registers
* Program count to indicated what to execute?
	- CPU register holding address of next instruction
* A “thread of execution”
	- able to execute instructions
	- has its own stack
	- each process has at least 1 thread of execution
* A process thread executes instructions found in the process’s address space ...
	- usually the text segment
* ... until a trap or interrupt
	- Time slice expires (timer interrupt)
	- Another event (e.g., interrupt from other device)
	- Exception (program error)
	- System call (switch to kernel mode)

### Program Creation System Calls
* *fork()*
	- Copy address space of parent and all threads
* *forkl()*
	- Copy address space of parent and only calling thread
* *vfork()*
	- Do not copy the parent’s address space
	- Share address space between parent and child
* *exec()*
	- Load new program and replace address space
	- Some resources may be transferred (open file descriptors)
	- Specified by arguments

### Process Creation with New Program
<img src="https://github.com/missystem/cis415review/blob/master/fork.png"><br />
* **Parent process** calls *fork()* to spawn child process
	- Both parent and child return from fork()
	- Continue to execute the same program
* **Child process** calls *exec()* to load a new program

### C Program Forking Separate Process
```
int main( )
{
	pid_t pid;
	// fork another process
	pid = fork( );
	if (pid < 0) { // error occurred
		fprintf(stderr, "Fork Failed"); exit(-1);
	} 

	else if (pid == 0) { /* child process */
		execlp("/bin/ls", "ls", NULL); // exec a file
	} 

	else { 		/* parent process */
		// parent will wait for the child to complete
		wait(NULL);
		printf ("Child Complete"); 
		exit(0);
	} 	
}
```
```
execl, execlp, execle, execv, execvp, execvpe
```
all execute a file and are frontends to execve <br />

### Process Layout
<img align="left" width="350" height="350" src="https://github.com/missystem/cis415review/blob/master/process_layout.png"> <br /><br />

1. PCB with new PID created 
2. Memory allocated for child initialized by copying over from the parent
3. If parent had called wait(), it is moved to a waiting queue 
4. If child had called exec(), its memory is overwritten with new code and data
5. Child added to ready queue and is
all set to go now
<br /><br />Effects in memory after parent calls fork()
<br /><br /><br /><br /><br />

### Analogy
* Doing a fork() is analogous to cloning
	- Copy is exactly the same
	- Completely able to live independently
	- Each has its own ability to execute
	- Each has its own resources
		- gets a copy of the parent’s process address space (unless you use another variant of fork())
		- gets a copy of the parent’s PCB (includes its memory management information)
	- Still executes the same code
* What if you want to change the clone’s behavior
* Doing an exec() is analogous to replacing the brain
	- Executing new program code

### Process Termination
* Process executes last statement and asks the OS to delete it (via *exit()*)
	- Output data from child to parent (via *wait()*)
	- Process' resources are deallocated by OS
* Parent may terminate execution of children processes (via *abort()*) if:
	- Child has exceeded allocated resources
	- Task assigned to child is no longer required
	- Parent is exiting
		- some OS do not allow child to continue if parent terminates
		- all children terminated - cascading termination

### Relocatable Memory
* Program instructions use addresses that logically start at 0
* Cannot place all programs in memory at physical address 0
* Relocation is the mechanism needed that enables the OS to place a program in an arbitrary location in memory
	- Gives the programmer the impression that they own the processor and the memory
* Program is loaded into memory at specific locations
	- Need some form of address translation (relocation) to do this
		- base-limit, segmentation, paging, virtual memory
* Also may need to share program code across processes

### Process State
* What do we need to track about a process?
	- How many processes?
	- What’s the state of each of them?
* How to track them?
	- Process table
		- Kernel data structure tracking processes on system
	- Process control block (PCB)
		- Structure for tracking process context

### Process Execution State
* As a process executes, it changes state
	- New:		  The process is being created (starting state)
	- Running:	  Instructions are being executed
	- Waiting:	  The process is waiting for some event to occur
	- Ready:      The process is waiting to run
	- Terminated: The process has finished execution (end state)
<img width="621" height="261" src="https://github.com/missystem/cis415review/blob/master/process_state.png">

### Process State
* consists of:
	- [Address space](https://github.com/missystem/cis415review/blob/master/lecturenotes03.md#process-address-space) (what can be addressed by a process)
	- Execution state (what is need to execute on the CPU)
	- Resources being used (by the process to execute)
* Address space contains code and data of a process
* Processes are individual execution contexts
	- Threads are also include here
* Resources are physical support necessary to execute
	- Memory: physical memory, address translation support
	- Storage: disk, files, ...
	- Processor: CPU (at least 1)

<img width="621" height="261" src="https://github.com/missystem/cis415review/blob/master/process_state.png">

### Process State 
* Running
	- Process is executing in the processor and in memory with all resources to run
* Ready
	- Process in memory with all resources to run, but is waiting for dispatch onto a processor
* Waiting
	- Process is not running, instead waiting for some event to occur

### State Transitions
* New Process ==> Ready
	- Allocate resources necessary to run
	- Place process on process queue (usually at end)
* Ready ==> Running
	- Process is at the head of process queue
	- Process is scheduled onto an available processor
* Running ==> Ready
	- Process is interrupted
		- usually by a timer interrupt
	- Process could still run, in that it is not waiting something
	- Placed back on the process queue

### State Transitions: Page Fault Handling
* Running ==> Waiting
	- Either something exceptional happened that caused an interrupt to occur (e.g., page fault exception) <br /> 
	or the process needs to wait on some action (e.g., it made a system call or requested I/O)
	- Process must wait for whatever event happened to be serviced
* Waiting ==> Ready
	- Event has been satisfied so that the process can return to run
	- Put it on the process (ready) queue
* Ready ==> Running

### State Transitions: Other Issues
* Priorities
	- Can provide policy indicating which process should run next
* Yield
	- System call to give up processor voluntarily
	- For a specific amount of time (sleep)
* Exit
	- Terminating signal (Ctrl-C)

### Process Control Block (PCB)
* also called the *task control block*
* contains information (metadata) associated with a specific process kept by the OS (kernel)
* including:
	- Process state:
		- new, ready, running, waiting, halted, etc
	- Program counter
		- indicates the address of the next executed process's instruction
	- CPU registers
		- for process thread
		- vary in number and type, depending on the computer architecture
	- CPU-scheduling information
		- includes a process priority, pointers to scheduling queues, and any other scheduling parameters
	- Memory-management information
		- memory allocated to the process
	- Accounting information
		- the amount of CPU and real time used, time limits, account numbers, job or process numbers, etc
	- I/O status information
		- the list of I/O devices allocated to the process
		- a list of open files, etc
* OS maintains a list of PCBs for all processes
* Process state lists link in the PCBs

<img width="254" height="392" src="https://github.com/missystem/cis415review/blob/master/PCB.png"> <img width="325" height="150" src="https://github.com/missystem/cis415review/blob/master/PCB_list.png">

### Per Process Control Information
* Process state
	- Ready, running, waiting (momentarily)
* Links to other processes
	- Children
* Memory Management
	- Segments and page tables
* Resources
	- Open files
* And much more...

### [*/proc*](https://github.com/missystem/cis415review/blob/master/lecturenotes02.md#sysfs-file-and-proc-files) File System
* Linux and Solaris
	- *ls /proc*
	- Process information pseudo-file system
	- Does not contain “real” files, but runtime system information
		- system memory
		- devices mounted
		- hardware configuration
	- A directory for each process
* A directory for each process
	- */proc/<pid>/io* <br />
		I/O statistics
	- */proc/<pid>/environ* <br />
		Environment variables (in binary)
	- */proc/<pid>/stat* <br />
		Process status and info

### Context Switch
* OS switches from one execution context to another
	- One process to another process
	- Interrupt handling
	- Process to kernel (mode transition, not context switch)
* Current process to new process
	- Save the state of the current process
		- PCB: describes the state of the process in the CPU
	- Load the saved context for the new process
		- load the new process’s process control block into OS and registers
	- Start the new process
* Does this differ if we are running an interrupt handler?
<img width="600" height="494" src="https://github.com/missystem/cis415review/blob/master/context_switch_process_to_process.png">

### Context Switch Performance
* No useful work is being done during a context switch <br />
	(i.e., no user process is running)
	- Want to speed up context switch processing
	- If a system call can be done in user mode, then the OS does not have to context switch
* Hardware support helps in context switching
	- Multiple hardware register sets
	- Be able to quickly set up the processor
* However, hardware optimization may conflict
	- Managing address translation tables is difficult
	- Different virtual to physical mappings on different processes

### Process Scheduling
* Maximize CPU use, quickly switch processes onto CPU for time sharing
* Process scheduler selects among available processes for next execution on CPU
* Maintains scheduling queues of processes
	- Job queue 
		- set of all processes in the system
	- Ready queue
		- set of all processes residing in main
	- Device queues
		- set of processes waiting for an I/O device
	- Processes migrate among the various queues

### Representation of Process Scheduling
* Process scheduling queueing diagram represents:
	- queues, resources, flows
* Processes move through the queues
<img width="534" height="304" src="https://github.com/missystem/cis415review/blob/master/process_scheduling.png">

### Ready Queue And Various I/O Device Queues
<img width="497" height="432" src="https://github.com/missystem/cis415review/blob/master/ready_IO_queues.png">

### Schedulers
* **Short-term scheduler** (process scheduler)
	- Selects which process should be executed next
	- Allocates CPU to running process
* **Medium-term scheduler** (multiprogram scheduler)
	- Manages process (jobs) in execution
	- Moves partially executed jobs to/from disk storage
	- Adjusts the degree of multiprogramming
* **Long-term scheduler** (job scheduler)
	- Selects which jobs should be allowed to run
	- Loads process and makes it ready to run

### Process Actions in Client-Server
* Example of forking to create a new process 
* Consider a web server <br />
<img width="116" height="60" src="https://github.com/missystem/cis415review/blob/master/process_actions_in_Client_Server_01.png"> <br />
* A remote “client” wants to connect and look at a webpage hosted by the web server <br />
<img width="300" height="150" src="https://github.com/missystem/cis415review/blob/master/process_actions_in_Client_Server_02.png"> <br />
* An interprocess communication is made and a connection is established <br />
<img width="300" height="150" src="https://github.com/missystem/cis415review/blob/master/process_actions_in_Client_Server_03.png"> <br />
* What if the web server only served this client until it was done looking at web pages?
* How can the web server support multiple “concurrent” clients?
* Create more “concurrency” by creating more processes! <br />
<img width="450" height="145" src="https://github.com/missystem/cis415review/blob/master/process_actions_in_Client_Server_04.png"> <br />
* A fork() copies the parent PCB and establish a child process initialized with that PCB
* All of the parent’s state is inherited by the child, including the connection to the client
* Responsibility for “servicing” the client is handed off to the “child” server process <br />
<img width="630" height="145" src="https://github.com/missystem/cis415review/blob/master/process_actions_in_Client_Server_05.png"> <br />
* The “parent” server process goes back to waiting for new clients
* The “parent” server should disengage from the client by releasing its (duplicate) connection <br />
<img width="450" height="150" src="https://github.com/missystem/cis415review/blob/master/process_actions_in_Client_Server_06.png"> <br />
* The “child” server is now completely responsible for the client connection
* There is still a logical relationship between the child / parent server processes
* Communication takes places between the client and “child” server process <br />
<img width="300" height="150" src="https://github.com/missystem/cis415review/blob/master/process_actions_in_Client_Server_07.png"> <br />
* The “parent” server process is not involved
* Now, this can procedure can continue as new client connections come in <br />
<img width="450" height="240" src="https://github.com/missystem/cis415review/blob/master/process_actions_in_Client_Server_08.png"> <br />
* In this way, the amount of “concurrency” increases with more server processes <br />
<img width="620" height="270" src="https://github.com/missystem/cis415review/blob/master/process_actions_in_Client_Server_09.png"> <br />
* It gives both logical isolation between the servers, as well as simplifies the design and functionality
* Many types of servers operate this way
* A objective is to avoid blocking in the server
	- If one server is stalled, others can run <br />
<img width="630" height="330" src="https://github.com/missystem/cis415review/blob/master/process_actions_in_Client_Server_10.png">

## Summary
* Process
	- Execution state of a program
* Process Creation 
	- fork and exec
	- From binary representation
* Process Description
	- Necessary to manage resources and context switch
* Process Scheduling
	- Process states and transitions among them 
* Interprocess Communication
	- Ways for processes to interact (other than normal files)

## Summary from book
* A **process** is a program in execution
	- the status of the current activity of a process is represented by the program counter, as well as other registers.
* The **layout of a process in memory** is represented by 4  sections:
	1. text
	2. data
	3. heap
	4. stack
* As a process executes, it changes **state** 
	- 4 general states of a process: 
		1. ready
		2. running
		3. waiting
		4. terminated
* **Process Control Block (PCB)**
	- the kernel data structure that represents a process in an OS
* **Process scheduler**
	- select an available process to run on a CPU
* An operating system performs a **context switch** when it switches from running one process to running another
* **Create processes**
	- fork() is used in Linux system

---

## [Lecture 04: Interprocess Communication (Chapter 3)](https://github.com/missystem/cis415review/blob/master/lecture-4-ipc.pdf)

## Outline
* [Interprocesses Communication (IPC)](https://github.com/missystem/cis415review/blob/master/lecturenotes04.md#interprocess-communication-ipc)
* [Sockets](https://github.com/missystem/cis415review/blob/master/lecturenotes04.md#sockets)
* [Remote Procedure Calls (RPC)](https://github.com/missystem/cis415review/blob/master/lecturenotes04.md#remote-procedure-calls-rpc)
* [Summary](https://github.com/missystem/cis415review/blob/master/lecturenotes04.md#summary)


### Process Communication
* Process cooperation is a fundamental aspect of an OS and of the computing environment it is maintaining on behalf of executing applications
* Processes need to interact and share information
* Process model is a useful way to isolate running programs <br />
(separate resources, state, and so on)
	- It can simply programs (no need to worry about other processes)
	- But processes do not always work in isolation, nor do we want them to
	- Sometimes it is easier to design programs if there are multiple processes working together
* Why provide environment that allows process cooperation?
	- Information sharing
	- Computation speedup
	- Modularity

### Process Communication (Interoperation)
* When is communication necessary?
* Examples in OS:
	- Kernel/OS access to user process data
	- Processes sharing data via shared memory
	- Processes sharing data via system calls
	- Processes sharing data via file system
	- Threads with access to same data structures
* In general, there are numerous examples in computer science where interoperation is important
	- DB transactions, distributed computing, parallelism


### Interprocess Communication (IPC)
* Mechanism for processes to communication and synchronize
* Logically, we want some sort of messaging system
* Logically, an IPC facility would provide 2 operations for processes to communicate:
	- Send(message) (message size fixed or variable)
	- Receive(message)

* However, first there needs to be a communication channel
	- If processes P and Q wish to communicate they open a channel
	- Then exchanging messages can proceed

* How is the communication channel (link) realized?
	- Physically, there are different alternatives
	- Logically, need abstract interfaces, protocols, and properties

### IPC Mechanisms
* Allow cooperating processes to exchange data
* 2 fundamental methods (OS supported):
	- Shared memory
		- Pipes, Shared buffer
		- A region of memory that is shared by the cooperating processes is established
		- Processes can then exchange information by reading and writing data to the shared region
		- Faster than message passing
	- Message passing
		- Mailboxes, Sockets
		- Communication takes place by means of messages exchanged between the cooperating processes
		- Useful for exchanging smaller amounts of data (no conflicts need be avoided)
		- Easier to implement in a distributed system than shared memory
* Choosing depends on the application
<img width="553" height="353" src="https://github.com/missystem/cis415review/blob/master/communication_models.png">

### IPC in Shared-Memory Systems
* Producer-Consumer Problem
	- Producer writes
	- Consumer reads <br />
<img width="345" height="90" src="https://github.com/missystem/cis415review/blob/master/producer_consumer.png"> <br />
* Producer action
	- While the buffer not full ... stuff can be written (added) to the buffer
* Consumer actions
	- When stuff is in the buffer ... it can be read (removed)
* Must manage where new stuff is in the buffer
* 2 types of buffers
	- Unbounded buffer (Realistic?)
		- Variable in size (no practical limit on the size of the buffer)
		- The consumer may have to wait for new items
		- The producer can always produce new items
	- Bounded buffers (Problems?)
		- Fixed in size
		- The consumer must wait if the buffer is empty
		- The producer must wait if the buffer is full

### Shared Memory -- Producer
```
item next_produced;			
							// Circular buffer
while (true) {
	/* produce an item in next produced */

	while (((in + 1) % BUFFER_SIZE) == out) 
		; /* do nothing ... buffer is full */

	buffer[in] = next_produced;
	in = (in + 1) % BUFFER_SIZE;
}
```

### Shared Memory -- Consumer
```
item next_consumed;

while (true) {
	while (in == out)
        ; /* do nothing ... buffer is empty */
	
	next_consumed = buffer[out]; 
	out = (out + 1) % BUFFER_SIZE;

	/* consume the item in next_consumed */
}
```

### IPC with Shared Memory
* Communicate by reading/writing from a specific memory location (in logical memory)
	- Setup a shared memory region (segment) in your process
	- Permit others to attach to the shared memory region
* [*shmget(2)*](http://man7.org/linux/man-pages/man2/shmget.2.html) -- create shared memory segment 
	- Permissions (read and write)
	- Size
	- Returns an identifier for the segment
* [*shmat(2)*](http://man7.org/linux/man-pages/man2/shmat.2.html) -- attach to existing shared memory segment
	- Specify identifier
	- Location in local address space 
	- Permissions (read and write)
* Also, operations for detach and control

### IPC with Pipes
* Producer-Consumer mechanism for data exchange
	- prog1 | prog2 (shell notation for pipe)
	- Output of prog1 becomes the input to prog2
	- Precisely, a connection is made so that the stdout of prog1 is connected to stdin of prog2
* OS sets up a fixed-size buffer (OS manages it)
	- System calls: [*pipe(2)*](http://man7.org/linux/man-pages/man2/pipe.2.html), [*dup(2)*](http://man7.org/linux/man-pages/man2/dup.2.html), [*popen(2)*](http://man7.org/linux/man-pages/man3/popen.3.html)
* Producer
	- Write to buffer, if space available
* Consumer
	- Read from buffer if data available

### Management of Pipes
* Buffer management
	- A finite region of memory (array or linked-list)
	- Wait to produce if no room
	- Wait to consume if empty
	- Produce and consume complete items
* Access to buffer
	- Write adds to buffer (updates end of buffer)
	- Reader removes stuff from buffer (updates start of buffer) 
	- Both are updating buffer state
* Issues
	- What happens when end is reached (e.g., in finite array)? 
	- What happens if reading and writing are concurrent?
	- Who is managing the pipe?

### IPC in Message-Passing Systems
* Mechanisms for processes to communicate and to synchronize actions
* Messaging system
	- Processes communicate without shared variables
	- Use messages instead
* Establish communication link
	- Producer sends on link
	- Consumer receives on link
* IPC Operations
	- *send(P, message)*: send a message to process P
	- *receive(Q, message)*: receive a message from process Q
* Communication link logical implementations
	- Direct or indirect communication
	- Synchronous or asynchronous communication 
	- Automatic or explicit buffering
* Issues
	- What if a process wants to receive from any proces?
	- What if communicating processes are not ready at same time?
	- What size message can a process receive?
	- Can other processes receive the same message from one process?

### Synchronous Messaging
* Direct communication from one process to another
* Synchronous send
	- *send(P, message)*
	- Producer must wait for the consumer to be ready to receive the message
* Synchronous receive
	- *receive(ID, message)*
	- ID could be any process
	- Wait for someone to deliver a message
	- Allocate enough space to receive message
* Synchronous means that both have to be ready
	- Otherwise, process (sender or receiver) is blocked

### Properties of Direct Communication Links
* Links are established automatically
* A link is associated with exactly one pair of communicating processes
* Between each pair there exists exactly one link
* The link may be unidirectional, but is usually bi-directional
* Messaging
	- Processes must name each other explicity (Naming)

### Asynchronous Messaging
* Indirect communication from one process to another
* Asynchronous send
	- *send(M, message)*
	- Producer sends message to a buffer M (like a mailbox)
	- No waiting needed (modulo busy mailbox)
* Asynchronous receive
	- *receive(M, message)*
	- Receive a message from a specific buffer (get your mail)
	- No waiting (modulo busy mailbox)
	- Allocate enough spacee to receive message
* Asynchronous means that can send/receive when it's ready
* What are some issues with the buffer?

### Properties of Indirect Communication Link
* Link established only if processes share a mailbox
* A link may be associated with many processes
* Each pair of processes may share several communication links
* Link may be unidirectional or bi-directional
* Messages
	- Messages are directed and received from mailboxes (also referred to as ports)
	- Each mailbox has a unique ID
	- Processes can communicate only if they share a mailbox

### Synchronization
* Message passing may be either *blocking* or *non-blocking*
* **Blocking** is considered synchronous
	- Blocking send
		- sender is blocked until the message is received
	- Blocking receive
		- receiver is blocked until a message is available
* **Non-blocking** is considered asynchronous
	- Non-blocking send
		- sender sends the message and continue
	- Non-blocking receive
		- receiver receives: a valid message or a null message
* Different combinations possible
	- If both send and receive are blocking, we have a rendezvous

### Communication in Client–Server Systems
* Sockets
* Remote procedure calls (RPC)
* Remote method invocation (a la Java)

### Sockets
* An endpoint for communication
* Sockets are named by
	- IP address (roughly, machine)
	- Port number (service: ssh, http, ...)
	- Concatenate the two (161.25.19.8:1625)
* A pair of processes communicating over a network employs a pair of sockets (one for each process)
	- Connect one socket to another (TCP/IP)
	- Send/receive message to/from another socket (UDP/IP)
* Semantics
	- Bidirectional link between a pair of sockets
	- Messages: unstructured stream of bytes
* Connection between
	- Processes on same machine (UNIX domain sockets)
	- Processes on different machines (TCP or UDP sockets)
	- User process and kernel (netlink sockets)
<img width="640" height="440" src="https://github.com/missystem/cis415review/blob/master/communication_models.png"> <br />

### Files and File Descriptors (POSIX)
* POSIX system calls for interacting with files
	- *open(), read(), write(), close()*
	- *open()* returns a *file descriptor*
		- an integer that represents an open file
		- inside the OS, it's an index into a table that keeps track of any state associated with you interactions, such as the file position
	- you pass the file descriptor into *read, write*, and *close*
	- File descriptors are kept as part of the process information in the process control block

### Networks and Sockets
* UNIX likes to make all I/O look like file I/O
	- Can use *read()* and *write()* to interact with remote computers over a network
* File descriptors are used for network communications
	- The socket is the file descriptor
* Just like with files
	- Your program can have multiple network channels (sockets) open at once
	- You need to pass *read()* and *write()* the socket file descriptor to let the OS know which network channel you want to use
* Examples of Sockets
	- HTTP / SSL
	- email (POP/IMAP)
	- ssh
	- telnet

### IPC and Sockets
<img width="675" height="430" src="https://github.com/missystem/cis415review/blob/master/IPCandSockets.png">

### File Descriptors
<img width="775" height="480" src="https://github.com/missystem/cis415review/blob/master/fileDescriptor.png">

### Type of Sockets
* Stream sockets
	- For connection-oriented, point-to-point, reliable bytestreams
		- uses TCP, SCTP, or other stream transports 
* Datagram sockets
	-  For connection-less, one-to-many, unreliable packets
		- uses UDP or other packet transports
* Raw sockets
	- For layer-3 communication
		- raw IP packet manipulation

### Stream sockets
* Typically used for client / server communications 
	- also for other architectures (peer-to-peer)
* Client
	- An application that establishes a connection to a server
* Server 
	- An application that receives connections from clients <br />
<img width="295" height="85" src="https://github.com/missystem/cis415review/blob/master/streamSocket01.png"><br /><img width="295" height="85" src="https://github.com/missystem/cis415review/blob/master/streamSocket02.png"><br /><img width="290" height="75" src="https://github.com/missystem/cis415review/blob/master/streamSocket03.png"><br />

### Datagram Sockets
* Used less frequently than stream sockets
	- They provide no flow control, ordering, or reliability
* Often used as a building block
	- Streaming media applications
	- Sometimes, DNS lookups
<br /><img width="320" height="200" src="https://github.com/missystem/cis415review/blob/master/datagramSockets01.png"><br /><img width="335" height="185" src="https://github.com/missystem/cis415review/blob/master/datagramSockets02.png"><br />

### Issues using Sockets
* Communication semantics
	- Reliable or not
* Naming
	- How do we know a machine’s IP address? DNS
	- How do we know a service’s port number?
* Protection
	- Which ports can a process use?
	- Who should you receive a message from?
		- Services are often open -- listen for any connection
* Performance
	- How many copies are necessary?
	- Data must be converted between various data types

### Remote Procedure Calls (RPC)
<br /><img width="513" height="617" src="https://github.com/missystem/cis415review/blob/master/RPC.png"><br />
* message-based communication scheme
* Each message is addressed to an RPC daemon listening to a **port** on the remote system, and each contains an identifier specifying the function to execute and the parameters to pass to that function
	- port: a \# included at the start of a msg packet
* The function is then executed as requested, and any output is sent back to the requester in a separate message
* Supported by systems
	- CORBA
	- Java RMI
* Issues
	- Support to build client/server stubs
	- Marshaling arguments and code
	- Layer on existing mechanism (e.g., sockets) 
	- Remote server crashes ... then what?
* Performance vs. abstractions
	- What if the two processes are on the same machine?
* Marshaling
<br /><img width="660" height="365" src="https://github.com/missystem/cis415review/blob/master/marshaling.png"><br />

### Remote Method Invocation (RMI)
* RMI is a Java mechanism similar to RPCs
* RMI allows a Java program on one machine to invoke a method on a remote object
<br /><img width="735" height="295" src="https://github.com/missystem/cis415review/blob/master/RMI.png"><br />

### IPC Summary
* Lots of mechanisms
	- Pipes
	- Shared memory 
	- Sockets
	- RPC
* Trade-offs
	- Ease of use, functionality, flexibility, performance
Implementation must maximize these
	- Minimize copies (performance)
	- Synchronous vs Asynchronous (ease of use, flexibility)
	- Local vs Remote (functionality)

## Summary
* **Shared memory**
	- Two (or more) processes share the same region of memory
* **Message passing**
	- Two processes communicate by exchanging messages with one another
* **Pipe**
	- provides a conduit for two processes to communicate
	- 2 forms of pipes:
		- Ordinary pipes
		- Named pipes
* 2 common tyeps of **client–server communication**
	- **sockets**
		- allow two processes on different machines to communicate over a network
	- **remote procedure calls (RPCs)**
		- abstract the concept of function (procedure) calls in such a way that a function can be invoked on another process that may reside on a separate computer

---

## [Lecture 05: Threads (Chapter 4)](https://github.com/missystem/cis415review/blob/master/lecture-5-threads.pdf)

## Outline
* [Why thread? - Advantages of threads](https://github.com/missystem/cis415review/blob/master/lecturenotes05.md#advantages-of-threads)
* [Why thread? - Problem solved with threads](https://github.com/missystem/cis415review/blob/master/lecturenotes05.md#problems-solved-with-threads)
* [Why not thread?](https://github.com/missystem/cis415review/blob/master/lecturenotes05.md#why-not-threads)
* [Threads vs. Processes](https://github.com/missystem/cis415review/blob/master/lecturenotes05.md#threads-vs-processes-difference)
* [Thread Models](https://github.com/missystem/cis415review/blob/master/lecturenotes05.md#many-to-one-thread-model)
* [Pthreads](https://github.com/missystem/cis415review/blob/master/lecturenotes05.md#posix-threads)
* [Summary](https://github.com/missystem/cis415review/blob/master/lecturenotes05.md#summary)

### Program with Multiple Processes
<img width="811" height="487" src="https://github.com/missystem/cis415review/blob/master/prog_with_multiple_processes.png">

### [Program Creation System Calls](https://github.com/missystem/cis415review/blob/master/lecturenotes03.md#program-creation-system-calls)
* fork()
	- Copy address space of parent and all threads
* vfork()
	- Do not copy the parent’s address space
	- Share address space between parent and child
	- While parent blocks, child must exit or call exec
* exec()
	- Load new program and replace address space
	- Some resources may be transferred (open file descriptors)
	- Specified by arguments
* clone()
	- Like fork() but child shares some process context
	- More explicit control over what is shared
	- Process address space can be shared!
	- Calls a function pointed to by argument

### Process Model
*  Much of the OS’s job is keeping processes from interfering with each other
* Each process has its OWN resources to use
	- Program code to execute, address space, files, ...
* Processes are good for isolation (protection)
	- Prevent one process from affecting another process
* Processes are *heavyweight*
	- Pay a price for isolation
	- A full “process swap” is required for multiprocessing 
	- There is lots of process state to save and restore
	- OS must context switch between them 
		- intervene to save/restore all process state
* Is there an alternative?

### Why Threads?
* A process is “a program in execution”
	- Memory address space containing code and data
	- Other resources (e.g., open file descriptors)
	- State information (PC, register, SP) => PCB details
* Consider a process in 2 respects (categories) 
	1. Collection of resources required for execution
		- code, address space, open files, ...
	2. A “thread of execution”
		- current state of execution (CPU state) 
		- where the execution is now (PC)
* Suppose we think about these separately!
	- Resources
	- Execution state

### Terminology
* Multiprogramming
	- Running multiple programs on a computer concurrently
	- Each program is one process or a set of processes
	- Processes of different programs are independent
* Multiprocessing
	- Running multiple processes on a computer concurrently 
	- OS manages mapping of processes to processor(s)
* Multithreading
	- Define multiple execution contexts (threads of execution) in a single address space (of a process)
	- OS manages mapping of threads to an address space
	- OS manages mapping of threads to processor(s)

### What’s a Thread?
* A basic unit of CPU utilization; 
* It comprises 
	- a thread ID
	- a program counter (PC)
	- a register set
	- a stack
* Thread of execution through a program on a CPU
	- Program counter					(*per thread*)
	- Registers							(*per thread*)
* Memory
	- Address space						(*process*)
		- address space is *shared*
	- Stack 							(*per thread*)
		- each thread has its own stack pointer
	- Heap <br />						(*process*)
		\+ private dynamic memory 		(*per thread*)
* I/O
	- Share files, sockets, ...			(*process*)

### Why Multithreaded Applications?
* Multiple threads sharing a common address space
	- More correctly, they share process resources, including memory
* Why would you want to do that?
	- Some applications could be written to support concurrency
	- How do you get concurrency?
		- One way is to create multiple processes
		- Use IPC to support process-level concurrency
* Some applications want to share data structures among concurrently executing parts of the computation
	- Is this possible with processes? => shared segments
	- Is it difficult with processes? => well, it is not that easy
	- Again, use IPC for process-level data sharing
* What is the problem? What is the solution?

### Advantages of Threads
* Threads could be used if there is no need for the OS to enforce resource separation
	- This is a trust issue, in part (back off on isolation / protection)
	- However, introduces more issues with respect to concurrency
* Benefits
	* Responsiveness (improved)
		- Possible to have a thread of execution that never blocks
	* Resource sharing (is facilitated)
		- All threads in a process have equal access to resources
	* Economy (of resources)
		- Thread-level resources are “cheaper” than process resources!
		- Threads are “lighter weight”
	* Utilization of multiprocessors (Scalability)
		- Run multiple threads on multiple processes without the overhead of running multiple processes

### Single-Threaded vs. Multithreaded
* Regular UNIX process can be considered as a special case of a multithreaded process
	- A process that contains just one thread
* Multithreaded process has multiple threads
<img width="1328" height="604" src="https://github.com/missystem/cis415review/blob/master/singleThreaded_multithreaded.png">

### Working with Threads
*  In a C program
	- *main()* procedure defines the first thread 
	- C programs always start at *main()*
* Now create a second thread
	- Allocate resources to maintain a second execution context in same address space
		- think about what state will be needed for a thread
	- Want something similar to *fork()* but simplere
		- supply a procedure name when start the new thread
	- Remember this creates another thread of execution

### Threads vs. Processes (difference)
* Easier to create than a new process
* Less time to terminate a thread than a process
* Less time to switch between two threads 
	- within the same process
* Less communication overheads
	- communicating between the threads of one process is simple because the threads share everything 
	- address space is shared
	- thus memory is shared

### Which is Cheaper?
* Create new process or create new thread (in existing process)?
* Context switch between processes or threads? 
* Interprocess or Interthread communication?
* Sharing memory between processes or threads?
* Terminating a process or terminating a thread (not the last one)?
* Time to create 100,000 processes (Linux 2.6 kernel, x86-32 system) <br />
(vfork(): faster fork) <br />
clone() creates a lightweight Linux process (thread) <br />

| Process creation method | Time (sec), elapsed (real) |
| -----------------------:|:--------------------------:|
| fork()                  | 22.27 (7.99)               |
| vfork()                 | 3.52 (2.49)                |
| clone()                 | 2.97 (2.14)                |

### Implications?
* Consider a web server on a Linux platform
* Measure 0.22 ms per *fork()*
	- Maximum of (1000 / 0.22) = 4545.5 connections/sec
	- 0.45 billion connections per day per machine
		- fine for most servers
		- too slow for a few super-high-traffic front-line web services
* Facebook serves O(750 billion) page views per day
	- Guess \~1-20 HTTP connections per page
	- Would need 3,000 -- 60,000 machines just to handle fork(), without doing any work for each connection!
* What is the problem here?

### Thread Attributes

| Global to process     | Local to specific thread    |
|:--------------------- |:--------------------------- |
| - memory              | - thread ID                 |
| - PID, PPID, GID, SID | - stack                     |
| - controlling term    | - signal mask               |
| - process credentials | - thread-specific date      |
| - record locks        | - alternate signal stack    |
| - FS information      | - error return value        |
| - timers              | - scheduling policy/priority|
| - resource limits     | - Linux-specific (e.g., CPU affinity)|

### Threading Models
* *Programming* - library or system call interface
	- Kernel threading (most common)
		- thread management support in the kernel
		- invoked via system call
		- NOTE: CPU only runs kernel threads
	- User-space threading
		- thread management support in user-space library
		- threads are "thread switched" by a user-level library
		- linked into you program
* *Scheduling* - application or kernel scheduling
	- May create user-level or kernel-level threads

### Kernel Threads
* Thread management support in kernel
	- Sets of system calls for creating, invoking, and switching among threads
* Supported and managed directly by the OS
	- Thread objects in the kernel
* Nearly all OSes support a notion of threads
	- Linux -- thread and process abstractions are mixed
	- Solaris
	- Mac OS X
	- Windows XP
	- ...

### User-space Threads
* Thread management support in user-space library
	- Sets of functions for creating, invoking, and switching among threads (all in user mode)
	- Need to switch threads stacks in user space ...
* Linked into your program
	- Thread libraries
* Examples:
	- [Qthreads](http://www.cs.sandia.gov/qthreads/)
	- [GNU Pth](http://www.gnu.org/software/pth/)
	- [Cilk](https://en.wikipedia.org/wiki/Cilk)

### Implementing User-space Threading
* Threads can perform operations in user mode that are usually handled by the OS
	- Assumes cooperating threads so hardware enforcement of separation not required
* Idea:
	- Think of a “dispatcher” subroutine in the process that is called when a thread is ready to relinquish control to another thread
	- Manages stack pointer, program counter
	- Switches process’s internal state among threads

### Many-to-One Thread Model
* Many user-level threads correspond to a single kernel thread
	- Kernel is not aware of the mapping
	- Handled by a thread library <br />
	<img width="570" height="330" src="https://github.com/missystem/cis415review/blob/master/manytoone.png"> <br />
* How does it work?
	- Create and execute a new thread
	- Upon yield, switch to another user thread in the same process
		- kernel is unaware
	- Upon wait, all threads are blocked 
		- kernel is unaware there are other options
		- can not wait and run at the same time

### One-to-One Thread Model
* One user-level thread per kernel thread
	- A kernel thread is allocated for every user-level thread
	- Must get the kernel to allocate resources for each new user- level thread <br />
	<img width="570" height="330" src="https://github.com/missystem/cis415review/blob/master/onetoone.png"> <br />
* How does it work?
	- Create new thread
		- system call to kernel
	- Upon yield, switch to another kernel thread in system
		- kernel is aware
	- Upon wait, another thread in the process may run
		- only the single kernel thread is blocked
		- kernel is aware there are other options in this process

### Many-to-Many Thread Model
* A pool of user-level threads maps to a pool of kernel threads
	- Pool sizes can be different (kernel pool is no larger)
	-  A kernel thread is pool is allocated for every user-level thread
	- No need for the kernel to allocate resources for each new user-level thread <br />
	<img width="570" height="330" src="https://github.com/missystem/cis415review/blob/master/manytomany.png"> <br />
* How does it work?
	- Create new thread
		- may map to kernel thread dynamically
	- Upon yield, switch to another thread 
		- kernel is aware
	- Upon wait, another thread in the process may run
		- if a kernel thread is available to be scheduled to that process
		- kernel is aware of the mapping between user and kernel threads

### Two-Level Model
<img width="580" height="320" src="https://github.com/missystem/cis415review/blob/master/twolevel.png"> <br />

### Problems Solved with Threads
* Imagine you are building a web server
* Problem:
	- A web server accepts client requests for web pages, images, sound, and so forth. A busy web server may have several (perhaps thousands of) clients concurrently accessing it. If the web server ran as a traditional single-threaded process, it would be able to service only one client at a time, and a client might have to wait a very long time for its request to be serviced.
* Solution: 
	- You could allocate a pool of threads, one for each client
		- thread would wait for a request, get content file, return it 
	- How would the different thread models impact this?
* Imagine you are building a web browser
	- You could allocate a pool of threads 
		- some for user interface
		- some for retrieving content
		- some for rendering content
	- What happens if the user decided to stop the request? ◆
		- mouse click on the stop button

### Multithreaded Server Architecture
<img width="1010" height="370" src="https://github.com/missystem/cis415review/blob/master/MultithreadedServerArchitecture.png"> <br />

### Linux Threads
* Linux uses one-to-one thread model
	- Threads are called tasks
* Linux views threads as “contexts of execution”
	- Threads are defined separately from processes
	- There is flexibility in what is private and shared
* Linux system call
	- *clone(int (\*fn)(), void \*\*stack, int flags, int argc, ...)*
	- Create a new thread (Linux task)
* May be created in the same address space or not
	- Flags (on means “share”): clone VM, clone filesystem, clone files, clone signal handlers
	- If all these flags off, what system call is clone equal to?

### POSIX Threads
* POSIX Threads is a thread API specification
	- Does not define directly the implementation
	- Could be implemented differently
* A POSIX standard (IEEE 1003.1c) API for thread creation and synchronization
	- Specification, not implementation
	- Provided either as user-level or kernel-level (\*)
	- API specifies behavior of the thread library
	- Implementation is up to development of the library
* Common in UNIX operating systems
	- Solaris, Linux, Mac OS X
* POSIX Threads is also known as *Pthreads*

### POSIX Threads Functions
* start the thread
	```pthread_create()```
* return thread ID
	```pthread_self()```
* for comparisons of thread ID's
	```pthread_equal()```
* return from the start function
	```pthread_exit()```
* wait for another thread to terminate & retrieve value from pthread_exit()
	```thread_join()```
* terminate a thread, by TID
	```pthread_cancel()```
* thread is immune to join or cancel & runs independently until it terminates
	```pthread_detach()```
* thread attribute modifiers
	```pthread_attr_init()```

### POSIX Threads FAQ
* How to pass multiple arguments to start a thread? 
	- Build a struct and pass a pointer to it
* Is the pthreads ID unique to the system?
	- No, just process – Linux task ids are system-wide
* After pthread_create(), which thread is running? 
	- It acts like fork in that both threads are running
* How many threads terminate when ...
	- exit() is called? – all in the process
	- pthread_exit() is called? – only the calling thread
* How are variables shared by threads? 
	- Globals, local static, dynamic data (heap)

### Figure 4.11 Multithreaded C program using the Pthreads API.
```
#include <pthread.h>
#include <stdio.h> #include <stdlib.h>

int sum; 	/* this data is shared by the thread(s) */
void *runner(void *param); /* threads call this function */

int main(int argc, char *argv[])
{
	pthread_t tid; 		/* the thread identifier */
	pthread_attr_t attr; 	/* set of thread attributes */

	/* set the default attributes of the thread */ 
	pthread_attr_init(&attr);

	/* create the thread */
	pthread_create(&tid, &attr, runner, argv[1]);

	/* wait for the thread to exit */ 
	pthread_join(tid,NULL);

	printf("sum = %d∖n",sum); 
}


/* The thread will execute in this function */ 
void *runner(void *param)
{
	int i, upper = atoi(param);
	sum = 0;

    for (i = 1; i <= upper; i++)
    	sum += i;

    pthread_exit(0);
}
```

### Figure 4.12 Pthread code for joining ten threads.
```
#define NUM_THREADS 10

/* an array of threads to be joined upon */ 
pthread_t workers[NUM THREADS];

for (int i = 0; i < NUM THREADS; i++) 
	pthread_join(workers[i], NULL);
```

### Concurrency with Threads
* Consider the client-server example again
* Now want to run with just a single process
	- Need to use threads to get concurrency
* Process "main" (parent) thread waits for clients
	- Parent thread forks (or dispatches) a new thread to handle each new client connection
	- Responsibilities of the child thread:
		- handles the new connection
		- exits when the connection terminates

### Client-Server with Pthreads
<img width="330" height="390" src="https://github.com/missystem/cis415review/blob/master/client_server_pthreads_01.png">
<img width="580" height="390" src="https://github.com/missystem/cis415review/blob/master/client_server_pthreads_02.png">
<img width="580" height="390" src="https://github.com/missystem/cis415review/blob/master/client_server_pthreads_03.png">
<img width="580" height="390" src="https://github.com/missystem/cis415review/blob/master/client_server_pthreads_04.png">
<img width="580" height="390" src="https://github.com/missystem/cis415review/blob/master/client_server_pthreads_05.png">
<img width="580" height="390" src="https://github.com/missystem/cis415review/blob/master/client_server_pthreads_06.png">

### Implications?
* Consider a web server on a Linux platform
* 0.0297 ms per thread create (time for clone()) 
	- 10x faster than process forking
	- Maximum of (1000 / 0.0297) = \~33,670 connections/sec
	- 3 billion connections per day per machine
		- much, much better
* Facebook would need only 500 machines 
* So, why do we need processes at all?<br />
Just write everything using threads<br />
	- Writing safe multithreaded code can be complicated 
	- Why? What are the issues?

### Why not threads?
* Threads can interfere with one another
	- Impact of more threads on caches
	- Impact of more threads on TLB
	- Bug in one thread can lead to problems in others
* Executing multiple threads may slow them down 
	- Impact of single thread vs. switching among threads
* Harder to program a multithreaded program 
	- Multitasking hides context switching
	- Multithreading introduces concurrency issues


### Concurrent Threads
* Benefits
	- All threads are running the same code
		- still the case that much of the code is identical!
	- Shared-memory communication is possible
	- Good CPU and network utilization 
		- lower overhead than processes
* Disadvantages
	- Synchronization is complicated
	- Shared fate within a process
		- one rogue thread can hurt you badly

### Inter-Thread Communication
* Can you use shared memory?
	- YES
	- just need to allocate memory in the address space
		- No need for fancy IPC shared memory
* Can you use message passing
	- YES
	- would have to build infrastructure
* Can threads utilize IPC mechanisms
	- would need to make sure only 1 thread uses at a time

### Fork/Exec Issues
* Semantics are ambiguous for multithreaded processes
* fork()
	- How does it interact with threads?
* exec()
	- What happens to the other threads?
* fork, then exec
	- Should all threads be copied?

### Thread Pools
* General Idea:
	1. Create a number of threads at start-up 
	2. Place them into a pool
		- where they sit and wait for work
	3. When a server receives a request
		- it submits the request to the thread pool and resumes waiting for additional requests
	4. If there is an available thread in the pool
		- it is awakened, and the request is serviced immediately
	5. If the pool contains no available thread
		- the task is queued until one becomes free
	6. Once a thread completes its service
		- it returns to the pool and awaits more work. 
	7. Thread pools work well when the tasks submitted to the pool can be executed asynchronously
* Benefits:
	1. Servicing a request with an existing thread is often faster than waiting to create a thread
	2. A thread pool limits the number of threads that exist at any one point. 
		- important on systems that cannot support a large number of concurrent threads
	3. Separating the task to be performed from the mechanics of creating the task allows us to use different strategies for running the task
		- For example, the task could be scheduled to execute after a time delay or to execute periodically.
* Pool of threads
	- Create (all) at initialization time
	- Assign task to a waiting thread
		- it is already made so it should be fast
	- Use all available threads
* If the task is done
	- Suppose another request is in the queue
	- should we use running thread or another thread?
* Concern for the setup time cost
* Faster than setting up a process, but what is necessary?
	- How do we improve performance?

### Signal Handling
* What’s a signal?
	- A form of IPC
	- Send a particular signal to another process
* Receiver’s signal handler processes signal on receipt
* Example
	- Tell the Internet daemon (*inetd*) to reread its config file
	- Send signal to inetd: *kill -SIGHUP \<\pid>*
	- inetd’s signal handler for the SIGHUP signal re-reads the config file
* Note: some signals cannot be handled by the receiving process, so they cause default action (kill the process)
* Synchronous signals
	- Generated by the kernel for the process
	- Due to an exception -- divide by 0
		- events caused by the thread receiving the signal
* Asynchronous signals
	- Generated by another process
* Asynchronous signals are more difficult for multithreading

### Signal Handling and Threads
* Send a signal to a process <br />
Which thread should it be delivered to? -- It depends...
* Choices
	- Thread to which the signal applies 
	- Every thread in the process
	- Certain threads in the process
	- A specific signal receiving thread
* UNIX signal model created decades before Pthreads
	- Conflicts arise as a defult
* Synchronous vs. asynchronous cases
* Synchronous
	- Signal is delivered to the same process that caused the signal
	- Which thread(s) would you deliver the signal to?
* Asynchronous
	- Signal generated by another process
	- Which thread(s) in this case?


### Thread Cancellation
* 2 choices to stop a thread from executing
	- Synchronous cancellation
		- wait for the thread to reach a point where cancellation is permitted
		- no such operation in Pthreads, but can create your own
	- Asynchronous cancellation
		- terminate it now
		- *pthread_cancel(thread_id)*

### Terms
* *Reentrant* code
	- Code that can be run by multiple threads concurrently
* *Thread-safe* libraries
	- Library code that permits multiple threads to invoke the safe function
	- Mainly concerned with variables that should be private to individual threads
	- Requires moving some global variables to local variables
* Requirements
	- Rely only on input data
		- Or thread-specific data
	- Must be careful about locking (later)

### Thread Assignment (Scheduling)
* How many kernel threads to create for a process? 
	- In an M:N model there can be significantly more user-level threads (M) than kernel-level threads (N)
	- Kernel threads are the “real” threads that can be allocated to a CPU (core) and run
	- Want to keep enough kernel threads to satisfy the desired level of concurrency in a program and activity in the system
* Suppose that all kernel threads except 1 are blocked
	- What happens if the last kernel thread blocks?
	- Does it matter if there are more user threads “ready” to run?
* In multiprocessing, thread affinity is the notion of assigning a thread to run on a particular CPU
	- Help to improve the thread’s performance by keeping its execution resources (CPU, cache, memory) local to CPU

### Scheduler Activation
* It would be nice if the kernel told the application that a kernel thread was blocking
	- Scheduler activation
		- at thread block, the kernel tells the application via an upcall
		- an upcall is a general term for an invocation of application function from the kernel
	- User-level thread scheduler can then get a new user-level thread created
* Way of conveying information between the kernel and the user-level thread scheduler regarding the disposition of:
	- \# user-level threads (increase or decrease)
	- User-level thread state
		- running to waiting, waiting to ready

---

## Summary
* Threads
	- A mechanism to improve performance and CPU utilization
* Kernel-space and user-space threads
	- Kernel threads are real, schedulable threads
	- User-space may define its own threads (but not real)
* Threading models and implications
	- Programming systems
	- Multi-threaded design issues
* Useful, but not a panacea
	- Slow down system in some cases 
	- Can be difficult to program
* Multiprogramming and multithreading are important concepts in modern operating systems

## Summary from book
* **thread** 
	- a basic unit of CPU utilization
	- threads belonging to the same process share many of the process resources
		- including code and data
* There are 4 primary benefits to multithreaded applications: 
	1. responsiveness
	2. resource sharing
	3. economy
	4. scalability
* **Concurrency** 
	- multiple threads are making progress, whereas 
* **Parallelism** 
	- multiple threads are making progress simultaneously. 
* On a system with a single CPU, only concurrency is possible; parallelism requires a multicore system that provides multiple CPUs.
* challenges in designing multithreaded applications:
	- dividing and balancing the work
	- dividing the data between the different threads
	- identifying any data dependencies
	- test and debug (especially challenging)
* **Data parallelism** 
	- distributes subsets of the same data across different computing cores and performs the same operation on each core
* **Task parallelism** 
	- distributes tasks across multiple cores
	- each task is running a unique operation
* User applications create **user-level threads**
	- must ultimately be mapped to kernel threads to execute on a CPU
	- approaches:
		- many-to-one: maps many user-level threads to one kernel thread
		- one-to-one 
		- many-to-many
• **thread library** 
	- provides an API for creating and managing threads
	- 3 common thread libraries: Windows, Pthreads, Java threading
	- Windows
		- for the Windows system only
	- Pthreads 
		- available for POSIX-compatible systems such as UNIX, Linux, and macOS
	- Java threads 
		- run on any system that supports a Java virtual machine (JVM)
• **Implicit threading**
	- involves identifying tasks — not threads
	- allowing languages or API frameworks to create and manage threads
	- Approaches
		- thread pools
		- fork-join frameworks
		- Grand Central Dispatch
	- an increasingly common technique for programmers to use in developing concurrent and parallel applications.
• **Threads Termination**
	- Asynchronous cancellation
		- stops a thread immediately, even if it is in the middle of performing an update
	- Deferred cancellation
		- informs a thread that it should terminate but allows the thread to terminate in an orderly fashion
	- In most circumstances, deferred cancellation is preferred to asynchronous termination.
• Linux system
	- does not distinguish between processes and threads;
	- it refers to each as a task
	- *clone()* system call used to create tasks 
		- behave either more like processes or more like threads

---

## [Lecture 06: CPU Scheduling (Chapter 5)](https://github.com/missystem/cis415review/blob/master/lecture-6-scheduling.pdf)

## Outline
* [Basic Scheduling Concepts](https://github.com/missystem/cis415review/blob/master/lecturenotes06.md#basic-concepts--cpu-io-bursts)
* [Scheduling Criteria](https://github.com/missystem/cis415review/blob/master/lecturenotes06.md#scheduling-criteria) 
* [Scheduling Algorithms](https://github.com/missystem/cis415review/blob/master/lecturenotes06.md#scheduling-algorithms)
* [Thread Scheduling](https://github.com/missystem/cis415review/blob/master/lecturenotes06.md#thread-scheduling)
* [Multiple processor Scheduling](https://github.com/missystem/cis415review/blob/master/lecturenotes06.md#multiple-processor-scheduling)
* [Real-Time CPU Scheduling](https://github.com/missystem/cis415review/blob/master/lecturenotes06.md#real-time-scheduling)
* [Algorithm Evaluation](https://github.com/missystem/cis415review/blob/master/lecturenotes06.md#algorithm-evaluation)
* [Summary](https://github.com/missystem/cis415review/blob/master/lecturenotes06.md#summary)
---

### Resource Allocation
* In a multiprogramming / multiprocessing system, OS shares resources among running processes
	- There are different types of OS resources?
* Which process gets access to which resources and why?
	- To maximize performance
	- To increase utilization, throughput, responsiveness, ... 
	- Enforce priorities

### Resource Types
* Memory
	- Allocate portion of finite resource
	- Physical resources are limited
	- Virtual memory tries to make this appear infinite
* I/O
	- Allocate portion of finite resource and time spent with the resource
	- Store information on disk
	- A time slot to store that information
* CPU
	- Allocate time slot with resource
	- A time slot to run instructions

### Types of CPU Scheduling
* CPU resource allocation => **Scheduling**
* *Long-term scheduling* (admission)
	- determining whether to add to the set of processes to be executed
* *Medium-term scheduling*
	- determining whether to add to the number of processes partially or fully in memory (degree of multiprogramming)
* *Short-term scheduling*
	- determining which process will be executed by the processor
* *I/O scheduling*
	- determining which process’s pending I/O request will be handled by an available I/O device

### CPU Scheduling Views
* Single process view
	- GUI (graphical user interface) request
		- click on the mouse (*responsiveness*) 
	- Scientific computation
		- long-running, but want to complete ASAP (*time to solution*)
* System view (objectives)
	- Get as many tasks done as quickly as possible
		- *throughput* objective
	- Minimize waiting time for processes
		- *response time* objective
	- Get full utilization from the CPU
		- *utilization* objective

### Process Scheduling
* Process transition diagram
* OS perspective
<img width="707" height="353" src="https://github.com/missystem/cis415review/blob/master/processscheduling.png">

### When does scheduling occur?
* CPU scheduling decisions may take place when:
	1. A process switches from running -> waiting
	2. A process switches from running -> ready
	3. A process switches from waiting -> ready
	4. A process terminates
* Process voluntarily gives up (yields) the CPU
	- CPU scheduler kicks in an decides who to go next
	- Process gets put on the ready queue
	- It could get immediately re-scheduled to run
 
### Scheduling Problem
* Choose the ready/running process to run at any time
	- Maximize “performance”
* Model (estimate) “performance” as a function 
	- System performance of scheduling each process
		- *f(process) = y*
	- What are some choices for *f(process)*?
* Choose the process with the best y
	- Estimating overall performance is intractable
	- Scheduling so all tasks are completed ASAP is a NP-complete problem
	- Adding in preemption does not help

### Preemption
* Can we reschedule a process that is actively running (i.e., preempt its execution)?
	- If so, we have a preemptive scheduler
	- If not, we have a non-preemptive scheduler
* Suppose a process becomes ready
	- A new process is created or it is no longer waiting
* There is a currently running process
* However, it may be “better” (whatever this means) to schedule the process just put on the ready queue
	- So, we have to preempt the running process
* In what ways could the new process be better?

### Basic Concepts – CPU-I/O Bursts
* Maximum CPU utilization is obtained with multiprocessing ... Why?
* CPU–I/O burst cycle
	- Process execution consists of cycles of CPU execution and I/O wait
		- run instructions (CPU burst)
		- wait for I/O (I/O burst)
* CPU burst distribution
	- How much a CPU is used during a burst?
	- This is a main concern ... Why?
* Scheduling is aided by knowing the length of these bursts
<img width="240" height="430" src="https://github.com/missystem/cis415review/blob/master/cpu_IO_burst.png">

### Histogram of CPU Burst Times
* Profile for a particular process
<img width="650" height="425" src="https://github.com/missystem/cis415review/blob/master/histogram_of_cpu_burst.png">

### Dispatcher
* Dispatcher module gives control of the CPU to the process selected by the *short-term* scheduler
* This involves:
	- Switching context
		- save context of running process
		- loading process context of selected process to run
	- Switching to user mode
	- Jumping to the proper location in the user program to continue execution of the program
* *Dispatch latency*
	- time it takes for the dispatcher to stop one process and start another running
	- Context switch time

### Scheduling Criteria
* *Utilization / Efficiency*
	- Keep the CPU busy 100% of the time with useful work
* *Throughput*
	- Maximize the number of jobs processed per hour.
* *Turnaround time (latency)*
	- From the time of submission to the time of completion.
* *Waiting time*
	- Sum of time spent (in ready queue) waiting to be scheduled on the CPU
* *Response time*
	- Time from submission until the first response is produced (mainly for interactive jobs)
* *Fairness*
	- Make sure each process gets a fair share of the CPU

### Scheduling Algorithm Optimization Criteria
* Max CPU utilization 
* Max throughput
* Min turnaround time 
* Min waiting time
* Min response time

### Scheduling Algorithms
* Some may seem intuitively better than others
* But a lot has to do with the type of offered *workload* to the processor
* Best scheduling comes with best context of the tasks to be completed
	- Knowing something about the workload behavior is important
* Distinguish between non-preemptive and preemptive cases
	- This has to do with whether a running process can be stopped during its execution and put back on the ready queue so that another process can acquire the CPU and run

### Scheduling Algorithms
* First-come, First-serve (FCFS)
	- Non-preemptive
	- Does not account for waiting time (or much else) 
		- Convoy problem
* Shortest Job First (SJF)
	- May be preemptive
	- Optimal for minimizing waiting time (how?)
* Round Robin
* Priority Scheduling
* Multilevel Queue Scheduling

### First-Come, First-Served (FCFS)
* Serve the jobs in the order they arrive
* Managed with a FIFO queue
* Nonpreemptive
	- Process is run until it has to wait or terminates
	- OS can not stop the process and put it on ready queue
	- *Once the CPU has been allocated to a process, that process keeps the CPU until it releases the CPU, either by terminating or by requesting I/O*
* Simple and easy to implement
	- When a process is ready, add it to tail of ready queue, and serve the ready queue in FCFS order
* Fair
	- No process is starved out
	- Service order is immune to job size (does not depend)
	- It depends only on *time of arrival* <br />

* Example: <br />

| Process | Burst Time |
|:-------:|:----------:| 
|    P<sub>1</sub>   |     24     | 
|    P<sub>2</sub>   |      3     |
|    P<sub>3</sub>   |      3     |

* Burst time here represents the job’s entire execution time <br />
	- Suppose that the processes arrive in the order: P1, P2, P3
		- Assume processes arrive at the same time (e.g., time 0)
	- The Gantt chart for the schedule is: <br />
	<img width="500" height="75" src="https://github.com/missystem/cis415review/blob/master/FCFSex1.png"> <br />
		- Waiting time for P1 = 0; P2 = 24; P3 = 27
		- Average waiting time: (0 + 24 + 27) / 3 = 17
* Reduce waiting time
	- Suppose that the processes arrive in the order: P2, P3, P1
		- Assume that they arrive at the same time
	- The Gantt chart for the schedule is: <br />
	<img width="500" height="75" src="https://github.com/missystem/cis415review/blob/master/FCFSex2.png"> <br />
		- Waiting time for P1 = 6; P2 = 0; P3 = 3
		- Average waiting time: (6 + 0 + 3)/3 = 3
		- Much better than previous case ... Why?
		- *Convoy effect*: short processes gets placed behind long process in the scheduling order (all the other processes wait for the one big process to get off the CPU)

### Shortest-Job-First (SJF)
* Suppose we know length of *next* CPU burst
* Then us these lengths to schedule the process
	- Process with the shortest next CPU burst time goes first
* Two schemes:
	1. Nonpreemptive 
		- once CPU given to the process it cannot bepreempted until it completes its CPU burst
	2. Preemptive 
		- if a new process arrives with CPU burst length less than remaining time of current executing process, then preempt
		- known as the Shortest-Remaining-Time-First (SRTF)
* SJF is optimal
	- Gives minimum average waiting time for a set of processes
	- So we should always use it, right?
* It cannot be implemented at the level of CPU scheduling, as there is no way to know the length of the next CPU burst
* Example <br />

| Process | Arrival Time | Burst Time |
|:-------:|:------------:|:----------:|
|    P<sub>1</sub>   |      0.0     |      7     |
|    P<sub>2</sub>   |      2.0     |      4     |
|    P<sub>3</sub>   |      4.0     |      1     |
|    P<sub>4</sub>   |      5.0     |      4     |

* Nonpreemptive SJF
	- Scheduler makes a decision at the time when the next job is to be scheduled <br />
	<img width="500" height="115" src="https://github.com/missystem/cis415review/blob/master/SJFex1.png"> <br />
	- Average waiting time = (0 + 6 + 3 + 7) / 4 = 4
* Preemptive SJF <br />
	- Scheduler makes a decision at any time using preemption to stop the currently running process <br />
	<img width="520" height="105" src="https://github.com/missystem/cis415review/blob/master/SJFex2.png"> <br />
	- Average waiting time = (9 + 1 + 0 +2) / 4 = 3

### Determining Length of Next CPU Burst
* can only predict the length (do not know for sure)
	- We expect that the next CPU burst will be similar in length to the previous ones
	- By computing an approximation of the length of the next CPU burst, we can pick the process with the shortest predicted CPU burst
* The next CPU burst is generally predicted as an ***exponential average*** of the measured lengths of previous CPU bursts
	- *Given:* <br />
		*t<sub>n</sub> - actual length of the n-th CPU burst* <br />
		*τ<sub>n</sub> - predicted value for the n-th CPU burst* <br />
		*τ<sub>n+1</sub> - predicted value for the next CPU burst* <br />
	 	*α such that 0 ≤ α ≤ 1*
	- *Define:* <br />
		τ<sub>n+1</sub> =αt<sub>n</sub> +(1−α)t<sub>n</sub>
* What to set α to?
	- α = 0? Recent history does not count
	- α = 1? Only the last CPU burst counts
	- Commonly, α set to 1⁄2
* Since both α and (1 - α) are ≤ 1, each successive term has less weight than its predecessor
* SRTF is the preemptive version

### CPU Burst Prediction
* α = 1/2 and τ0 = 10 <br />
<img width="600" height="433" src="https://github.com/missystem/cis415review/blob/master/nextCPUburst.png"> <br />

### Example of Shortest-remaining-time-first
* Now we add the concepts of varying arrival times and preemption to the analysis

| Process | Arrival Time | Burst Time |
|:-------:|:------------:|:----------:|
|    P<sub>1</sub>   |      0     |      8     |
|    P<sub>2</sub>   |      1     |      4     |
|    P<sub>3</sub>   |      2     |      9     |
|    P<sub>4</sub>   |      3     |      5     |

* Preemptive SJF Gantt Chart <br />
<img width="500" height="75" src="https://github.com/missystem/cis415review/blob/master/SRTFex.png"> <br />
* Average waiting time <br />
= [(10 - 1) + (1 - 1) + (17 - 2) + (5 - 3)] / 4 <br />
= 26 / 4 <br />
= 6.5 msec <br />


### Round Robin (RR)
* Each process gets a small unit of CPU time (time quantum / time slice)
	- Usually 10-100 milliseconds
	- After this time has elapsed, the process is *preempted* and added to the end of the ready queue
* Approach
	- Consider *n* processes in the ready queue
	- Consider time quantum is *q*
	- Then each process gets *1/n* of the CPU time
	- In chunks of at most *q* time units at once
	- No process waits more than *(n - 1)q* time units
* Example of RR with Time Quantum = 4 <br />

| Process | Burst Time |
|:-------:|:----------:|
| P<sub>1</sub> | 24 |
| P<sub>2</sub> |  3 |
| P<sub>3</sub> |  3 |

* The Gantt chart <br />
<img width="500" height="75" src="https://github.com/missystem/cis415review/blob/master/RRex.png"> <br />
	- Typically, higher average turnaround than SJF, but better response times
	- *q* should be large compared to context switch time 
		- Usually *q* is between 10ms to 100ms
		- Context switch < 10 uses
* RR Time Quantum
	- Round robin allows the CPU to be virtually shared between the processes
		- Each process has the illusion that it is running in isolation (at *1/n*-th the CPU speed)
	- Smaller time quantums make this illusion more realistic, but there are problems
		- What is the main problem? <br />
		If context-switch time is added in, the average turnaround time increases even more for a smaller time quantum, since more context switches are required.
	- Larger time quantums will give more preference to processes with larger burst times
		- What scheduling algorithm is approximated when quantums are very large? <br />
		If the time quantum is too large, RR scheduling degenerates to an FCFS policy

* How a smaller time quantum increases context switches: <br />
<img width="480" height="210" src="https://github.com/missystem/cis415review/blob/master/RRcontextswitch.png"> <br />
* How turnaround time varies with the time quantum: <br />
<img width="410" height="340" src="https://github.com/missystem/cis415review/blob/master/RRturnaround.png"> <br />
* ==> 80 percent of the CPU bursts should be shorter than the time quantum
* If time quantum size decreases, what happens to the context switches?
	- Context switches are not free!
		- Saving/restoring registers
		- Switching address spaces
		- Indirect costs (cache pollution)

### Priority Scheduling
* Each process is given a certain priority “value”
* Always schedule the process with highest priority
	- Preemptive
	- Non-preemptive
* Problems:
	- Starvation (indefinit blocking)
		- Low priority processes may never execute 
* Solution:
	- Aging
		- gradually increasing the priority of processes that wait in the system for a long time
* FCFS and SJF are specialized versions of Priority Scheduling
	- Assigning priorities to the processes in a certain way
		- What would the priority function be for FCFS? 
		- What would the priority function be for SJF?

* Example of Priority Scheduling <br />

|      Process 		 | Burst Time |  Priority  |
|:------------------:|:----------:|:----------:|
|    P<sub>1</sub>   |     10     |      3     |
|    P<sub>2</sub>   |      1     |      1     |
|    P<sub>3</sub>   |      2     |      4     |
|    P<sub>4</sub>   |      1     |      5     |
|    P<sub>5</sub>   |      5     |      2     |

* Priority scheduling Gantt Chart (non-preemptive)<br />
<img width="495" height="75" src="https://github.com/missystem/cis415review/blob/master/PSex.png"> <br />
	- Average waiting time = 8.2 msec

### Scheduling Desirables
* SJF
	- Minimize waiting time
		- requires estimate of CPU bursts 
* Round robin
	- Share CPU via time quanta
		- if burst turns out to be “too long”
* Priorities
	- Some processes are more important
	- Priorities enable composition of “importance” factors
* No single best approach -> combinations

### Round Robin with Priority
* Have a ready queue for each priority level
* Always service the non-null queue at the highest priority level
* Within each queue, you perform round-robin scheduling between those processes
* Problems
	- With fixed priorities
		- Lower priority processes can get starved out
	- In general, you employ a mechanism to “age” the priority of processes
<img width="356" height="246" src="https://github.com/missystem/cis415review/blob/master/RRwithP.png"> <br />

### Multilevel Queue Scheduling
* Ready queue is partitioned into separate queues: 
	- foreground (interactive)
	- background (batch)
* These two types of processes have different response-time requirements and so may have different scheduling needs
* Each queue has its own scheduling algorithm: 
	- foreground – RR
	- background – FCFS
* Scheduling must be done between the queues 
	- Fixed priority scheduling
		- possibility of starvation 
	- Time slice
		- each queue gets a certain amount of CPU time which it can schedule amongst its processes
<img width="450" height="250" src="https://github.com/missystem/cis415review/blob/master/MQscheduling.png"> <br />

### Multilevel Feedback Queue
* Allow processes move between queues <br />
	*Aging* can be implemented this way
* A process uses too much CPU time will be moved to a lower-priority queue
* A process that waits too long in a lower-priority queue may be moved to a higher-priority queue
* Multilevel-feedback-queue scheduler defined by the following parameters:
	- Number of queues
	- Scheduling algorithms for each queue
	- Method used to determine when to upgrade a process
	- Method used to determine when to demote a process
	- Method used to determine which queue a process will enter when that process needs service
* Example <br />
<img width="350" height="210" src="https://github.com/missystem/cis415review/blob/master/MFQ.png"> <br />
* Three queues:
	- Q<sub>0</sub> – RR with time quantum 8 milliseconds
	- Q<sub>1</sub> – RR time quantum 16 milliseconds
	- Q<sub>2</sub> – FCFS
* Scheduling
	- A new job enters queue Q<sub>0</sub> which served FCFS
		- when it gains CPU, job receives 8 milliseconds
		- if it does not finish in 8 milliseconds, job is moved to queue Q<sub>1</sub>
	- At Q<sub>1</sub> job is again served FCFS and receives 16 additional milliseconds
		- if it still does not complete, it is preempted and moved to queue Q<sub>2</sub>

### Traditional UNIX Scheduling
* Multilevel Feedback Queues
* 128 priorities possible (-64 to +63)
* 1 Round Robin Queue per priority
* Every scheduling event the scheduler picks the highest priority (lowest number) non-empty queue and runs jobs in Round Robin

### UNIX Process Scheduling
* Negative numbers reserved for processes waiting in kernel mode
	- just woken up by interrupt handlers
	- why do they have a higher priority?
* Time quantum = 1/10 sec 
	- empirically found to be the longest quantum that could be used without loss of the desired response for interactive jobs such as editors
	- Short time quantum means better interactive response
	- Long time quantum means higher oeverall system throughput since less context switch overhead and less processor cache flush
* Priority dynamically adjusted to reflect
	- Resource requirement (e.g., blocked awaiting an event)
	- Resource consumption (e.g., CPU time)

### Linux Scheduler
* Kernel 2.4 and earlier: essentially the same as the traditional UNIX scheduler
* Kernel 2.6: O(1) scheduler
	- Time to select process is constant regardless of system load or the number of processors
	- Separate queue for each priority level
	- CPU affinity (keeps processes on same CPU)
* More recently (kernel 2.6.23 and up): CFS
	- Completely Fair Scheduler (runs O(log N))
		- uses red-black trees rather than runqueues

### Linux Scheduling
* Two algorithms:
	- time-sharing 
	- real-time
* Time-sharing (still abstracted)
	- Two queues: 
		- active 
		- expired
	- In active, until you use your entire time slice (quantum), then expired
		- once in expired, wait for all others to finish (fairness)
	- Priority recalculation -- based on waiting vs. running time 
		- from 0-10 milliseconds
		- add waiting time to value, subtract running time
		- adjust the static priority
* Real-time
	- Soft real-time
	- Posix.1b compliant – two classes
		- FCFS and RR (highest priority process always runs first)

### Priorities and Time-Slice Length
<img width="378" height="218" src="https://github.com/missystem/cis415review/blob/master/PandTSL.png"> <br />

### Thread Scheduling
* When threads are supported, it is threads that are scheduled, not processes
	- Distinction between user-level and kernel-level threads
* Many-to-one and Many-to-many models, thread library schedules user-level threads to run on LWP
	-  Known as process-contention scope (PCS) since scheduling competition is within the process
	- Typically done via priority set by programmer
* Kernel thread scheduled onto available CPU is system-contention scope (SCS) - competition among all threads in sytem

### Multiple-Processor Scheduling
* CPU scheduling is more complex with multiple CPUs
* Consider homogeneous processors within a multiprocessor
* *Asymmetric* multiprocessing
	- Only one processor accesses the system data structures, reducing the need for data sharing
	- Some parts of the OS only runs on this processor
* *Symmetric* multiprocessing (SMP)
	- Each processor is self-scheduling
		1. All threads may be in a common ready queue.
		2. Each processor may have its own private queue of threads
	- Currently, most common approach <br />
	<img width="405" height="210" src="https://github.com/missystem/cis415review/blob/master/multiprocessor.png"> <br />

### Multicore Processors
* Recent trend to place multiple processor cores on same physical chip
* Faster and consumes less power
* Multiple threads per core also growing 
	- Takes advantage of memory stall to make progress on another thread while memory retrieve happens

### Multithreaded Multicore System
* when a processor accesses memory, it spends a significant amount of time waiting for the data to become available
	- Memory stall occur
		- primarily because modern processors operate at much faster speeds than memory
		- can also because of a cache miss (accessing data that are not in cache memory)
<br /> <img width="455" height="120" src="https://github.com/missystem/cis415review/blob/master/singlecore.png"> <br />
* To remedy this situation, many recent hardware designs have imple- mented multithreaded processing cores in which two (or more) hardware threads are assigned to each core
* Multithreaded multicore system
<br /> <img width="470" height="125" src="https://github.com/missystem/cis415review/blob/master/mtmc.png"> <br />

### Load Balancing
* Need to keep all CPUs loaded for efficiency
* Load balancing keep the workload evenly distributed across all processors in an SMP system
* Push migration
	- periodically checks the load on each processor
		- if it finds an imbalance
		- evenly distributes the load by moving (or pushing) threads from overloaded to idle or less-busy processors
	- Pushes task from overloaded CPU to other CPUs
* Pull migration
	- Idle processors pulls waiting task from busy processor

### Processor affinity
* Process favors the processor on which it is currently running
	- soft affinity
		- OS has a policy of attempting to keep a process running on the same processor
		- but not guaranteeing that it will
	- hard affinity
		- allowing a process to specify a subset of processors on which it can run

### Real-time Scheduling
* *Hard real-time* systems
	- required to complete a critical task within a guaranteed amount of time
	- A task must be serviced by its deadline; service after the deadline has expired is the same as no service at all
* *Soft real-time* computing
	- requires that critical threads receive priority over less critical ones
	- provide no guarantee as to when a critical real-time process will be scheduled

* Minimizing Latency
* Priority-Based Scheduling
* Rate-Monotonic Scheduling
* Earliest-Deadline-First Scheduling
* Proportional Share Scheduling
* POSIX Real-Time Scheduling

### Algorithm Evaluation
* Maximizing CPU utilization under the constraint that the maximum response time is 300 milliseconds
* Maximizing throughput such that turnaround time is (on average) linearly proportional to total execution time

---

## Summary
* CPU Scheduling
	- Algorithms
	- Combination of algorithms
		- Multi-level Feedback Queues
* Scheduling Systems
	- UNIX
	- Linux

## Summary from book
* **CPU scheduling**
	- The task of selecting a waiting process from the ready queue 
	- Allocating the CPU to it
	- The CPU is allocated to the selected process by the dispatcher
* **Scheduling algorithms**
	- preemptive
		- where the CPU can be taken away from a process 
	- nonpreemptive
		- where a process must voluntarily relinquish control of the CPU
	- Almost all modern operating systems are preemptive
* Scheduling algorithms can be **evaluated** by 5 criteria: 
	1. CPU utilization
	2. throughput
	3. turnaround time
	4. waiting time
	5. response time.
* **First-come, first-served (FCFS) scheduling**
	- The simplest scheduling algorithm
	- May cause short processes to wait for very long processes
* **Shortest-job-first (SJF) scheduling**
	- Provably optimal
	- Providing the shortest average waiting time
	- Implementing is difficult because predicting the length of the next CPU burst is difficult
* **Round-robin (RR) scheduling**
	- Allocates the CPU to each process for a time quantum
		- If the process does not relinquish the CPU before its time quantum expires, the process is preempted, another process is scheduled to run for a time quantum
* **Priority scheduling**
	- Assigns each process a priority
	- The CPU is allocated to the process with the highest priority
	- Processes with the same priority can be scheduled in FCFS order or using RR scheduling
* **Multilevel queue scheduling**
	- Partitions processes into several separate queues arranged by priority
	- The scheduler executes the processes in the highest-priority queue
	- Different scheduling algorithms may be used in each queue.
* **Multilevel Feedback Queues**
	- similar to multilevel queues
	- except that a process may migrate between different queues
* **Multicore processors** 
	- place one or more CPUs on the same physical chip
	- each CPU may have more than one hardware thread
	- From the perspective of the operating system
		- each hardware thread appears to be a logical CPU
* **Load balancing** on multicore systems 
	- equalizes loads between CPU cores
	- migrating threads between cores to balance loads may invalidate cache contents
		- may increase memory access times
* **Soft real-time scheduling**
	- gives priority to real-time tasks over non-real-time tasks
* **Hard real-time scheduling**
	- provides timing guarantees for real-time tasks
* **Rate-monotonic real-time scheduling**
	- schedules periodic tasks using a static priority policy with preemption
* **Earliest-deadline-first (EDF) scheduling**
	- Assigns priorities according to deadline
	- The earlier the deadline the higher the priority
	- the later the deadline, the lower the priority.
* **Proportional share scheduling**
	- allocates *T* shares among all applications
	- If an application is allocated *N* shares of time, it is ensured of having *N∕T* of the total processor time
* **Completely fair scheduler (CFS)**
	- Linux uses it
	- Assigns a proportion of CPU processing time to each task
	- The proportion is based on the virtual runtime (```vruntime```) value associated with each task
* **Modeling and Simulations**
	- can be used to evaluate a CPU scheduling algorithm

---

## [Lecutre 07: Concurrency and Synchronization (Chapter 6/7)](https://github.com/missystem/cis415review/blob/master/lecture-7-synchronization.pdf)

## Outline
* Background
* [Critical-Section Problem](https://github.com/missystem/cis415review/blob/master/lecturenotes07.md#critical-section-problem-dijkstra-1965)
* [Peterson’s Solution](https://github.com/missystem/cis415review/blob/master/lecturenotes07.md#petersons-solution)
* [Synchronization Hardware](https://github.com/missystem/cis415review/blob/master/lecturenotes07.md#hardware-support-for-synchronization)
* [Mutual Exclusion (Mutex) Locks](https://github.com/missystem/cis415review/blob/master/lecturenotes07.md#mutex-locks)
* [Semaphores](https://github.com/missystem/cis415review/blob/master/lecturenotes07.md#semaphore-dijkstra)
* [Monitors](https://github.com/missystem/cis415review/blob/master/lecturenotes07.md#monitors)
* [Classic Problems of Synchronization](https://github.com/missystem/cis415review/blob/master/lecturenotes07.md#classical-problems-of-synchronization)
* [Chapter 6 Summary from OSC](https://github.com/missystem/cis415review/blob/master/lecturenotes07.md#chapter-6-summary-from-osc)
* [Chapter 7 Summary from OSC](https://github.com/missystem/cis415review/blob/master/lecturenotes07.md#chapter-7-summary-from-osc)

---

### Roadmap
<img width="408" height="250" src="https://github.com/missystem/cis415review/blob/master/roadmap.png">

### Concurrency and Synchronization
* Processes can execute concurrently (logically, physically)
	- May be interrupted at any time, partially completing execution
	- OS must concurrently execute its own software
* There are different kinds of resources that are shared between processes:
	- Physical (terminal, disk, network, ...)
	- Logical (files, sockets, memory, ...)
	- Memory
* Concurrent access to shared resources must be done in a consistent manner or else errors arise
* Maintaining data consistency requires mechanisms to ensure the orderly execution of cooperating processes
* This is the role of synchronization

### Resources
* Focus on “memory” as the shared resource for the purposes of this discussion
	- Processes can all read and write into memory 
	- Suppose multiple processes share memory
		- use IPC shared memory segments mechanisms
	- It might be easier to think of multiple threads 
		- sharing memory is natural

### Example Problem Due to Sharing
* Consider a shared printer queue
	- *```spoolQueue[n]```*
* 2 processes want to enqueue an element each to this queue
* *```tail```* points to the current end of the queue
	- It is also a shared variable
* Each process needs to do <br />
	```
	tail = tail + 1; 
	spoolQueue[tail] = "element";
	```

### What are trying to do?
* Want to have “consistent” and “correct” execution 
<br /><img width="355" height="125" src="https://github.com/missystem/cis415review/blob/master/spoolQueue.png"><br />
* What could go wrong?
	- ```tail = tail + 1``` is NOT a single machine instruction
		- So, what? Why do we care?
	- What assembly code does the compiler produce? <br />
	*Load tail, R1 <br />
	Add R1, 1, R2 <br />
	Store R2, tail <br />*
	- These 3 machine instructions might NOT be executed atomically ... Why not?
		- To *execute atomically* means to execute multiple instructions logically together as if they were a single instruction without being interrupted
	- What is the problem?

### Interleaving
* Each process executes this set of 3 instructions
* Interrupts might happen at any time
	- a context switch can happen at any time
* Suppose we have the following scenario:
<br /><img width="253" height="125" src="https://github.com/missystem/cis415review/blob/master/Interleaving.png"><br />
* Leading to incorrect execution:
<br /><img width="380" height="120" src="https://github.com/missystem/cis415review/blob/master/incorrectExecution.png"><br />
* ==> Race condition

### Race Conditions
* Several processes access and manipulate the same data concurrently and the outcome of the execution depends on the particular order in which the access takes place
* Race conditions are timing dependent
	- Errors can be non-repeatable
* Race conditions CAN cause the “state” of the execution to be inconsistent (incorrect)
	- It does not mean that because there is a race condition that the state WILL become inconsistent

### How to avoid race condition?
* Atomic: 
	- A set of instructions is **atomic** if it executed as if it was a single instruction (logically)
* What does this mean exactly?
	- Executing the instructions can not be interrupted?
	- It has more to do with the outcomes of executing the instructions with respect to other processes
* Suppose the 3 assembly instructions we were looking at were atomic
* Does this avoid the race condition?
* Critical Section: 
	- When executing a set of instructions is vulnerable to a race condition, that set of instructions are said to constitute a **critical section**
	- the process may be accessing — and updating — data that is shared with at least one other process

### Critical Section Problem (Dijkstra, 1965)
* Consider system of n processes {P<sub>0</sub>, P<sub>1</sub>, ... P<sub>n-1</sub>}
* Each process has critical segment of code
	- Process may be changing common variables, updating table, writing file, ... (in its critical section)
	- When one process is in its critical section, no other may be in its critical section (mutual exclusion)
* *Critical section problem* is to design a *protocol* between the processes to solve this
	- Each process must enter the critical section (entry section)
	- Each process then executes the critical section instructions 
	- Each process must exit the critical section (exit section)
	- Each process executes outside the critical section

### Critical Section
* General structure of process *P<sub>i</sub>*
<br /><img width="400" height="215" src="https://github.com/missystem/cis415review/blob/master/generalCriticalSectionStructure.png"><br />
* It is the entry and exit code that defines the critical section protocol
* The infinite do loop is just to suggest that a process will possibly want to enter its critical section multiple \#s of times, including never (0 times) or just once (1 time)

### Requirements for Solution to CS Problem
1. Mutual exclusion
	- If process *P<sub>i</sub>* is executing in its critical section, then no other processes can be executing in their critical sections
2. Progress
	- If no process is executing in its critical section and some processes wish to enter their critical sections, then only those processes that are not executing in their remainder sections can participate in deciding which will enter its critical section next, and this selection cannot be postponed indefinitely
3. Bounded waiting
	- There exists a bound, or limit, on the number of times that other processes are allowed to enter their critical sections after a process has made a request to enter its critical section and before that request is granted
	- Assume that each process executes at a nonzero speed
	- No assumption concerning relative speed of the *N* processes

### How to Implement Critical Sections
* Implementing critical section solutions follows 3 fundamental approaches
	1. Disable Interrupts
		- Effectively stops the scheduling of other processes
		- Does not allow another process to get the CPU
	2. Busy Waiting / Spinlock solutions
		- Pure software solutions
		- Integrated hardware-software solutions
	3. Blocking Solutions

### Disabling Interrupts
* We already know how to prevent another process from interrupting the current running process
	- Do not allow interrupts to occur by disabling them
* Advantages:
	- Simple to implement (single instruction)
* Disadvantages:
	- Do not want to give such power to user processes
	- Does not work on a multiprocessor ... Why?
		- Disabling interrupts on a multiprocessor can be time consuming, since the message is passed to all the processors
		- This message passing delays entry into each critical section, and system efficiency decreases
		- Also consider the effect on a system’s clock if the clock is kept updated by interrupts
	- Disables multiprogramming even if another process is NOT interested in critical section

### Busy Waiting (aka Spinning)
* Overall philosophy:
	- Keep checking some state (variables) until they indicate other process(es) are not in critical section
<br /><img width="453" height="227" src="https://github.com/missystem/cis415review/blob/master/busyWaiting.png"><br />
* Remember, P1 and P2 might do this multiple \#s of times, including 0.

### Reading, Writing, and Testing Locks
* Is the instruction below atomic?
	<br /> A: load locked, R1 
	<br />cmp R1, 1
	<br />beq A<br />
	```while (locked == TRUE)```  
* How about these instructions?
	```
	locked = TRUE;
	locked = FALSE;
	```
* Generally, if the high-level statement compiles to a single machine instruction, it is atomic
* Need reading, writing, testing of locks to be atomic

### Try Strict Alternation
* Consider this code (Left)
* Idea is to take turns using the critical section
	- Variable turn is used for this
* Does it work?
* What problems do you see? 
	- Is there mutual exclusion?
	- Is there progress?
	- Is there bounded waiting?  
* Remember, P1 and P2 might do this multiple #s of times, including 0

<br /><img width="275" height="412" src="https://github.com/missystem/cis415review/blob/master/strictAlternation.png"><img width="374" height="432" src="https://github.com/missystem/cis415review/blob/master/fixingProgress.png"><br />

### Fixing the “progress” requirement
* What about this code? (Right)
* Each process has a flag to say that they want to enter the critical section
* Got mutual exclusion
* Problems?
	- Deadlocked
* For this reason, it does NOT meet the progress or bounded waiting requirements either

### Peterson’s Solution
* Consider 2 processes
* Assume that the LOAD and STORE instructions are atomic and cannot be interrupted
* The two processes share two variables: 
	```
	int turn;
	boolean flag[2]
	```
* Variable turn indicates whose turn it is to enter the critical section
	- The ```flag``` array 
		- indicate if a process is ready to enter the critical section
		- ```flag[i] = true``` implies that process P<sub>i</sub> is ready
* Algorithm for Process Pi and Process Pj
<br /><img width="560" height="330" src="https://github.com/missystem/cis415review/blob/master/PetersonsSolution.png"><br />

### Does Peterson’s Solution work?
* Prove that the 3 CS requirements are met:
	- Mutual exclusion is preserved <br />
	P<sub>i</sub> enters CS only if:<br />
		- either ```flag[j] = false``` or ```turn = i```
		- if both processes are interested in entering, then 1 condition is false for one and true for the other process
	- Progress requirement is satisfied
		- a process wanting to enter will be able to do so at some point
		- Why?
	- Bounded-waiting requirement is met
		- eventually it will be P<sub>i</sub>’s turn if P<sub>i</sub> wants to enter
* Peterson’s solution **ONLY** works for 2 process solutions

### Multiple Processes
* How do we extend for multiple processes?
* Is there a way to modify Peterson’s approach?
* Consider the following enter /exit routines:
```
int turn;
int flag[N]; /* all set to FALSE initially */

enterCS(int myid) {
    otherid = (myid+1) % N;
    turn = otherid;
    flag[myid] = TRUE;

    while (turn == otherid && flag[otherid] == TRUE) ;
    /* proceed if turn == myid or flag[otherid] == FALSE */
}

leave_CS(int myid) {
	flag[myid] = FALSE;
}
```
* Remember, processes might do this multiple #s of times, including 0

### Bakery Algorithm (Leslie Lamport, 1974)
* We need to enforce a sequence in some manner that everyone will follow and contribute to making progress
* Think about a bakery
* See [A New Solution of Dijkstra's Concurrent Programming Problem](http://lamport.azurewebsites.net/pubs/bakery.pdf)
```
Notation: (a,b) < (c,d) if a<c  or  a=c and b<d

Every process has a unique id (integer) Pi

bool choosing[0..n-1]; /* all set to FALSE */ 
int number[0..n-1]; /

enter_CS(myid) { 
	choosing[myid] = TRUE; 
	number[myid] = max(number[0],number[1],...,number[n-1]) + 1;
	choosing[myid] = FALSE; 
	for (j=0 to n-1) {
	    while (choosing[j])
	      ;
	    while (number[j] != 0) && ((number[j],Pj)<(number[myid],myid))
	      ;
	}
}

leave_CS(myid) {
	number[myid] = 0;
}
```
* Evaluation of the Bakery Algorithm
	- Does it work?
	- What do you need to prove?
	- Show that it meets
		- Mutual exclusion
		- Progress
		- Bounded waiting requirements
	- Need to know that maximum \# processes

### Looking to the Hardware
* Complications arose because we had atomicity only at the granularity of a machine instruction
	- What a machine instruction could do is (was) limited
* Can we provide specialized instructions in hardware to provide additional functionality (with an instruction still being atomic)?
	- Looking again to hardware to help solve a OS problem q Many systems (now) provide hardware support for implementing the critical section code
* All solutions below are based on idea of locking
	- Protecting critical regions via locks
	- But without special instructions

### Hardware Support for Synchronization
* Uniprocessors – could disable interrupts
	- Currently running code executes without preemption
	- Generally too inefficient on multiprocessor systems 
		- OS using this not broadly scalable
* Modern CPUs provide atomic hardware instructions (2 general types)
	- **Test-and-Set**
		- test memory word and set value
	- **Compare-and-Swap**
		- swap contents of 2 memory words

### Critical-section Solution Using Locks
<br /><img width="170" height="165" src="https://github.com/missystem/cis415review/blob/master/lock.png"><br />
* The infinite while loop is just to suggest that a process will possibly want to enter its critical section multiple \#s of times, including 0 times and 1 time
* The problem before was that ```acquire``` and ```release``` could not be done in a single instruction
* Suppose it could with a single atomic instruction

### test_and_set instruction
* Definition
```
boolean test_and_set(boolean *target) { 
	boolean rv = *target;
	*target = true;
	return rv;
}
```
* Assume it executes atomically
* Returns the original value of passed parameter
* Set the new value of passed parameter to TRUE

### Solution using test_and_set()
* Shared boolean variable ```lock```, initialized to FALSE
* Mutual-exclusion implementation with ```test_and_set()```:
```
while (TRUE){ 
	...

	while (test_and_set(&lock) == TRUE) 
		;

		/* critical section */

	lock = FALSE;

		/* remainder section */
}
```
* if two test and set() instructions are executed simultaneously (each on a different core), they will be executed sequentially in some arbitrary order

### compare_and_swap Instruction
* Definition:
```
int compare_and_swap(int *value, int expected, int newvalue)
{
	int temp = *value;

	if (*value == expected)
		*value = newvalue;

	return temp;
}
```
* Assume it executes atomically
* Returns the original value of passed parameter value
* Set the variable ```value``` to the value of the passed parameter ```newvalue```
	- Only if ```value == expected```
* That is, the swap takes place only under this condition

### Solution using compare_and_swap
* Shared integer ```lock``` initialized to 0
```
while (TRUE) {
	...

	while(compare_and_swap(&lock, 0, 1) != 0)
		;	/* do nothing */

		/* critical section */

	lock = 0;

		/* remainder section */
}
```
* Does is work?
	- satisfies the mutual-exclusion requirement
	- does not satisfy the bounded-waiting requirement

### Bounded-waiting with test_and_set
* N processes: P<sub>0</sub>, P<sub>1</sub>, ..., P<sub>N-1</sub>
* This is the code for P<sub>i</sub> (i.e., each process is executing this code with *i* set to the process ID (*0 ≤ i ≤ N-1*)
* Bounded-waiting mutual exclusion with ```compare_and_swap()```
```
while (TRUE) {
	waiting[i] = TRUE;
	key = TRUE;
	while (waiting[i] && key == 1)
		key = compare and swap(&lock,0,1); 
	waiting[i] = false;

	    /* critical section */

	j = (i + 1) % n;
	while ((j != i) && !waiting[j])
		j = (j + 1) % n;
	
	if (j == i)
		lock = 0;
	else
		waiting[j] = false;
	
		/* remainder section */
}
```

### Spinning vs. Blocking
* In the previous solutions, we are spinning (busy-waiting) for some condition to change
	- This change should be effected by some other process
	- We are “presuming” that this other process will eventually get the CPU
		- is this a reasonable assumption?
		- needs a preemptive scheduler ... Why?
* This can be *inefficient* because:
	- wasting time quantum spinning
	- Sometimes, the programs may not work
		- suppose if the OS scheduler is not preemptive

### Blocking Approaches
* If instead of busy-waiting, the process relinquishes the CPU at the time when it cannot proceed, the process is said to **block**
	- It is still wanting to get into the critical section
	- It is put in the blocked queue
* It is the job of the process changing the *condition* to wake up a blocked process
	- Moves it from blocked back to ready queue
* Advantage:
	- Do not unnecessarily occupy CPU cycles 
* Disadvantages?

### Blocking Example
<br /><img width="390" height="165" src="https://github.com/missystem/cis415review/blob/master/BlockingExample.png"><br />
* Think of L as a lock
* NOTE: These must be OS system calls! Why?

### Mutex Locks
* Previous solutions are complicated and generally inaccessible to application programmers
* OS designers build software tools to solve critical section problem
* Simplest is a mutex lock (“mutual exclusion”)
* Protect a critical section by first ```acquire()``` a lock then
```release()``` the lock
	- Boolean variable indicating if lock is available or not
* Calls to ```acquire()``` and ```release()``` must be atomic
	- Usually implemented via hardware atomic instructions
* This solution generally requires busy waiting
	- This lock therefore is called a *spinlock*
* definition of ```aquire()```
```
acquire() {
	while (!available)
		; /* busy wait */
	available = false;
}
```
* definition of ```release```
```
release() { 
	available = true;
}
```
* Solution to the critical-section problem using mutex locks
	- Calls to either acquire() or release() must be performed atomically
```
while (TRUE) {
	acquire lock

		critical section

	release lock

		remainder section
}
```

### Synchronization Constructs
* Synchronization requires more than just exclusion
	- If printer queue is full, I need to wait until there is at least 1 empty slot
* Note that ```acquire()``` / ```release()``` are not very suitable to implement such synchronization ... Why?
* Main Disadvantage 
	- requires busy waiting
		- While a process is in its critical section, any other process that tries to enter its critical section must loop continuously in the call to acquire()
		- This continual looping is clearly a problem in a real multiprogramming system, where a single CPU core is shared among many processes
		- Busy waiting also wastes CPU cycles that some other process might be able to use productively
* We need constructs to enforce orderings
	- A should be done after B
* We need construction to help keep track of numbers of things

### Semaphore (Dijkstra)
* Synchronization tool that provides more sophisticated ways (than mutex locks) for processes to synchronize their activities
* A semaphore *S* is an integer variable
* Can only be accessed via two indivisible (atomic) operations
	- ```wait()``` and ```signal()```
	- Originally called P() and V() by Dijkstra <br />
	P = Probeer (‘try’ in Dutch)<br />
	V = Verhoog (‘increment’ in Dutch)<br />
* Definition of ```wait()```:
```
wait(S) {
	while (S <= 0)
		; // busy wait 
	S--;
}
```
* Definition of ```signal()```
```
signal(S) { 
	S++;
}
```
* [Semaphore (Wikipedia page)](https://en.wikipedia.org/wiki/Semaphore_(programming))

### Semaphore Usage
* *Binary semaphore*
	- Integer value can range only between 0 and 1
	- Same as a mutex lock
* *Counting semaphore*
	- Integer value can range over an unrestricted domain
* Can solve various synchronization problems with semaphores
* Consider P<sub>1</sub> and P<sub>2</sub> that require S<sub>1</sub> to happen before S<sub>2</sub>
	- Create a semaphore ```synch``` initialized to 0
	```
	P1:
		S1;
		signal(synch);
	P2:
		wait(synch);
		S2;
	```
* Can implement a counting semaphore *S* be a binary semaphore?
	- yes

### Semaphore Implementation
* Must guarantee that no two processes can execute the ```wait()``` and ```signal()``` ...
	- ... on the same semaphore ...
	- ... at the same time ...
* Thus, the implementation becomes the critical section problem where the ```wait``` and ```signal``` code are placed in the critical section
	- Could have busy waiting in critical section implementation
		- but implementation code is short
		- little busy waiting if critical section rarely occupied
* Note that applications may spend lots of time in critical sections and therefore this is not a particularly good solution

### Semaphores without Busy waiting
* With each semaphore there is an associated waiting queue
* Each entry in a waiting queue has two data items:
	- value (of type integer)
	- pointer to next record in the list
* Two operations:
	- ```block()```
		- place the process invoking the operation on the appropriate waiting queue
	- ```wakeup()```
		- remove one of processes in the waiting queue and place it in the ready queue
```
typedef struct{ 
	int value;
	struct process *list; 
} semaphore;

wait(semaphore *S) {
	S->value--;
	if (S->value < 0) {
		add this process to S->list;
		block(); 	// or sleep();
	}
}

signal(semaphore *S) {
	S->value++;
	if (S->value <= 0) {
		remove a process P from S->list;
		wakeup(P); 
	}
}
```
* NOTE: These are involving OS system calls, and there is no atomicity lost during the execution of these routines because interrupts are being disabled.

### Problems with Semaphores
* Incorrect use of semaphore operations: 
	- ```signal (mutex) .... wait (mutex)```
	- ```wait (mutex) ... wait (mutex)```
	- Omitting of ```wait (mutex)``` or ```signal (mutex)``` (or both)
* Deadlock and Starvation are possible
 
### Deadlock and Starvation
* Deadlock:
	- occurs if 2 or more processes are waiting indefinitely for an event that only one of the waiting processes can cause
	- Let S and Q be two semaphores initialized to 1
	<br /><img width="370" height="174" src="https://github.com/missystem/cis415review/blob/master/deadlock.png"><br />
* Starvation (indefinite blocking)
	- A process may never be removed from the semaphore queue in which it is suspended
* Priority inversion
	- Scheduling problem when lower-priority process holds a lock needed by higher-priority process
	- Solved via priority-inheritance protocol

### Monitors
* A high-level abstraction that provides a convenient and effective mechanism for process synchronization
* Abstract Data Type (ADT)
	- Internal variables only accessible by code within the procedure
	- Routines (operations) that operate on the internal variables (shared)
	- External world only sees these operations (not the shared data or how the operations and synchronization are implemented)
* Only one process may be active within the monitor at a time
	- All the processes that are executing monitor code, there can be at most 1 process in ready queue (rest are either blocked or not in monitor!)
* Pseudocode syntax of a monitor
```
monitor monitor name 
{
	/* shared variable declarations */

	function P1 ( . . . ) { 
		...
	}
	function P2 ( . . . ) { 
		...
	}
	function Pn ( . . . ) { 
		...
	}

		.
		.
		.
	initialization code ( . . . ) { 
		...
	} 
}
```

### Schematic view of a Monitor
<br /><img width="316" height="400" src="https://github.com/missystem/cis415review/blob/master/SchematicViewofMonitor.png"><br />

### Condition Variables
* ```condition x, y;```
* Two operations are allowed on a condition variable:
	- ```x.wait()```
		- a process that invokes the operation is suspended until ```x.signal()```
	- ```x.signal()```
		- resumes one of the processes (if any) that invoked ```x.wait()```
		- if no ```x.wait()``` on the variable, then it has no effect on the variable
* NOTE: If the ```signal``` comes before the ```wait```, the signal gets lost!!!
	- You need to be careful since signals are not stored unlike semaphores

### Monitor with Condition Variables
<br /><img width="430" height="303" src="https://github.com/missystem/cis415review/blob/master/MonitorwithConditionVariables.png"><br />

### Condition Variables Choices
* If process *P* invokes ```x.signal()```, and process *Q* is suspended in ```x.wait()```, what should happen next?
	- Both *Q* and *P* cannot execute in parallel
	- If *Q* is resumed, then *P* must wait
* Options include
	- Signal and wait
		- P waits until Q either leaves the monitor or it waits for another condition
	- Signal and continue
		- Q waits until P either leaves the monitor or it waits for another condition
	- Both have pros and cons – language implementer can decide
	- Monitors implemented in Jave, C#, Concurrent Pascal, ...

### Mesa versus Hoare Semantics
* Can implement signal in two ways
* *Mesa* (signal and continue)
	- Signal puts waiter on the ready list
	- Signaller keeps lock and the processor
	- Q either waits until P leaves the monitor or waits for another condition
* *Hoare* (signal and wait)
	- Signal gives processor and lock to waiter
	- Waiter gives processor and lock back to the signaller when it finishes
	- It is possible to support nested signalling
	- P either waits until Q leaves the monitor or waits for another condition

---

## Chapter 6 Summary from OSC
* **Race Condition**
	- occurs when processes have concurrent access to shared data 
	- the final result depends on the particular order in which concurrent accesses occur
	- can result in corrupted values of shared data
* **Critical Section**
	- a section of code where shared data may be manipulated 
	- a possible race condition may occur
	- The critical-section problem is to design a protocol whereby processes can synchronize their activity to cooperatively share data
* **Solution** to the critical-section problem must satisfy the following 3 requirements: 
	1. mutual exclusion
		- ensures that only one process at a time is active in its crit- ical section
	2. progress
		- ensures that programs will cooperatively determine what process will next enter its critical section
	3. bounded waiting
		- limits how much time a program will wait before it can enter its critical section
* Software solutions to the critical-section problem
	- Peterson’s solution
	- do not work well on modern computer architectures
* **Hardware support** for the critical-section problem includes:
	- memory barriers
	- hardware instructions
		- such as the compare-and-swap instruction
	- atomic variables.
* **Mutex Lock**
	- provides mutual exclusion by requiring that a process acquire a lock before entering a critical section and release the lock on exiting the critical section
* **Semaphores**
	- like mutex locks
	- can be used to provide mutual exclusion
	- However, whereas a mutex lock has a binary value that indicates if the lock is available or not, a semaphore has an integer value and can therefore be used to solve a variety of synchronization problems
* **Monitor**
	- an Abstract Data Type 
		- provides a high-level form of process synchronization
	- A monitor uses condition variables that allow processes to wait for certain conditions to become true and to signal one another when conditions have been set to true
* **Liveness Problem**
	- Solutions to the critical-section problem may suffer from liveness problems, including
		- deadlock
		- starvation
* The various tools that can be used to solve the critical-section problem as well as to synchronize the activity of processes can be evaluated under varying levels of contention. 
	- Some tools work better under certain contention loads than others

---

### Classical Problems of Synchronization
* Classical problems used to test newly-proposed synchronization schemes
	- Bounded-Buffer Problem
	- Readers and Writers Problem 
	- Dining-Philosophers Problem

### Bounded-Buffer Problem
* n buffers, each can hold one item
```int n```
* Semaphore ```mutex``` initialized to the value 1 
```semaphore mutex = 1;```
* Semaphore ```full``` initialized to the value 0
```semaphore full = 0```
* Semaphore ```empty``` initialized to the value n
```semaphore empty = n;```
<br /><br />
* The structure of the **producer** process 
```
while (TRUE) {
		...
	/* produce an item in next produced */ 
		...
	wait(empty);	/* what is known afterwards? */
	wait(mutex);

	/* --- start of critical section --- */
		...
	/* add next produced to the buffer */	
		...
	/* --- end of critical section --- */

	signal(mutex);	/* release CS access */
	signal(full);	/* informs the consumer */
}
```
<br /><br />
* The structure of the **consumer** process 
```
while (TRUE) {
		...
	/* produce an item in next produced */ 
		...
	wait(full);	/* what is known afterwards? */
	wait(mutex);

	/* --- start of critical section --- */
		...
	/* remove an item from buffer next_consumed */	
		...
	/* --- end of critical section --- */

	signal(mutex);	/* release CS access */
	signal(empty);	/* informs the consumer */
		...
	/* consume the item in next consumed */ ...
		...
}
```

### Readers-Writers Problem
* A data set is shared among a number of concurrent processes
	- Readers 
		- only read the data set (does not perform any updates)
	- Writers 
		- can both read and write
* Problem 
	- Allow multiple readers to read at the same time
	- Only one writer can access the shared data at the same time
* Several variations of how readers and writers are considered 
	- All involve some form of priorities
* Shared data
	- Dataset
	- Semaphore ```rw_mutex``` initialized to 1
	```semaphore rw mutex = 1;```
	- Semaphore ```mutex initialized``` to 1
	```semaphore mutex = 1;```
	- Integer ```read_count``` initialized to 0
	```int read_count = 0;```
<br /><br />
* The structure of a **writer** process
```
while (TRUE){ 
	wait(rw_mutex);

	/* --- start of critical section --- */
		...
		/* writing is performed */ 
		...
	/* --- end of critical section --- */

	signal(rw_mutex);
}
```
<br /><br />
* The structure of a **writer** process
```
while (TRUE){
	wait(mutex);
	read_count++;
	if (read_count == 1) 	/* what is happening here? */
		wait(rw_mutex); 
	signal(mutex);

	/* --- start of critical section --- */
		...
		/* reading is performed */ 
		...
	/* --- end of critical section --- */

	wait(mutex);
	read_count--;

	if (read_count == 0)
		signal(rw_mutex); 
	signal(mutex);
}
```
	- Can multiple readers be in critical section?
		- yes
	- Can writer be in critical section?
		- only one writer can be in critical section

### Readers-Writers Problem Variations
* Do you see any problems? 
* First variation (above)
	- No reader kept waiting unless writer has permission to use shared object
* Second variation
	- Once writer is ready, it performs the write ASAP 
	- How would you implement this?
* Both may have starvation leading to even more variations
* Problem is solved on some systems by kernel providing reader-writer locks

### Dining Philosophers Problem (Dijkstra)
* Philosophers spend their lives alternating thinking and eating
* When a philosopher thinks, she does not interact with her colleagues.
* When a philosopher gets hungry and tries to pick up the two chopsticks that are closest to her (the chopsticks that are between her and her left and right neighbors)
* A philosopher may pick up only one chopstick at a time
	- Need both to eat, then release both when done
* In the case of 5 philosophers
	- Shared data
		- bowl of rice (data set)
		- ```semaphore chopstick[5]``` initialized to 1
* [Wikipedia page for Dining Philosophers Problem](https://en.wikipedia.org/wiki/Dining_philosophers_problem)

### Dining Philosophers Problem Algorithm
* The structure of philosopher *i*
```
while (true) {
	wait(chopstick[i]); 
	wait(chopstick[(i + 1) % 5]);
		...
	/* eat for a while */ 		<-- critical section
		...
	signal(chopstick[i]);
	signal(chopstick[(i + 1) % 5]);
		...
	/* think for awhile */ 
		...
}
```

### Preventing Starving Philosophers
* Deadlock handling
	- Allow at most 4 philosophers to be sitting simultaneously at the table
		- cheating
	- Allow a philosopher to pick up the forks only if both are available
		- picking must be done in a critical section
	- Use an asymmetric solution
		- an odd-numbered philosopher picks up first the left fork and then the right fork
		- even-numbered philosopher picks up first the right fork and then the left fork

### Monitor Solution to Dining Philosophers
* A monitor solution to the dining-philosophers problem
```
monitor DiningPhilosophers
{
	enum {THINKING, HUNGRY, EATING} state[5];
	condition self[5];

	void pickup(int i) { 
		state[i] = HUNGRY; 
		test(i);
		if (state[i] != EATING)
		self[i].wait();
	}

	void putdown(int i) { 
		state[i] = THINKING; 
		test((i + 4) % 5); 
		test((i + 1) % 5);
	}

	void test(int i) {
		if ((state[(i + 4) % 5] != EATING)
			&& (state[i] == HUNGRY) 
			&&(state[(i + 1) % 5] != EATING)) 
		{
	    	state[i] = EATING;
			self[i].signal();
		} 
	}

	initialization code() {
		for (int i = 0; i < 5; i++)
			state[i] = THINKING;
	} 
}
```

### What does each Philosopher do?
* Each philosopher *i* invokes the operations ```pickup()``` and ```putdown()``` in the following sequence: <br />
	```
	DiningPhilosophers.pickup(i);
	EAT
	DiningPhilosophers.putdown(i);
	```
* No deadlock
* Starvation is possible

### Resuming Processes within a Monitor
* If several processes queued on condition x, and ```x.signal()``` executed, which should be resumed?
* FCFS frequently not adequate
* conditional-wait construct of the form ```x.wait(c)```
	- c is priority number
	- Process with lowest number (highest priority) is scheduled next

---

## Chapter 7 Summary from OSC
* Classic problems of process synchronization include 
	- bounded-buffer
	- readers–writers
	- dining-philosophers problems
	- Solutions to these problems can be developed using the tools presented in Chapter 6
		- mutex locks
		- semaphores
		- monitors
		- condition variables.
* Linux uses a variety of approaches to protect against race conditions, including atomic variables, spinlocks, and mutex locks.
* The POSIX API provides mutex locks, semaphores, and condition variables. POSIX provides two forms of semaphores: named and unnamed. Several unrelated processes can easily access the same named semaphore by simply referring to its name. Unnamed semaphores cannot be shared as easily, and require placing the semaphore in a region of shared memory.
* Alternative approaches to solving the critical-section problem include transactional memory, OpenMP, and functional languages. Functional languages are particularly intriguing, as they offer a different programming paradigm from procedural languages. Unlike procedural languages, functional languages do not maintain state and therefore are generally immune from race conditions and critical sections.

---

## [Lecture 08: Deadlocks (Chapter 8)](https://github.com/missystem/cis415review/blob/master/lecture-8-deadlocks.pdf)

## Outline
* [System Model](https://github.com/missystem/cis415review/blob/master/lecturenotes08.md#system-model)
* [Deadlock Characterization](https://github.com/missystem/cis415review/blob/master/lecturenotes08.md#deadlock-characterization)
* [Methods for Handling Deadlocks](https://github.com/missystem/cis415review/blob/master/lecturenotes08.md#methods-for-handling-deadlocks)
* [Deadlock Prevention](https://github.com/missystem/cis415review/blob/master/lecturenotes08.md#deadlock-prevention--requirements)
* [Deadlock Avoidance](https://github.com/missystem/cis415review/blob/master/lecturenotes08.md#deadlock-avoidance)
* [Deadlock Detection](https://github.com/missystem/cis415review/blob/master/lecturenotes08.md#deadlock-detection)
* [Recovery from Deadlock](https://github.com/missystem/cis415review/blob/master/lecturenotes08.md#deadlock-recovery--process-and-thread-termination)

---

### System Model
* System consists of resources
* Resource types R<sub>1</sub>, R<sub>2</sub>, . . ., R<sub>m</sub>
	- CPU cycles, memory space, I/O devices, ...
* Each resource type R<sub>i</sub> has W<sub>i</sub> instances
* Each process utilizes a resource as follows:
	- Request
	- Use
	- Release
* We want a general way to think about problems like the dining philosophers

### Deadlock Characterization
* Deadlock can arise if four conditions hold simultaneously: 
	1. <u>Mutual exclusion</u>
		- only one process at a time can use a resource
	2. <u>Hold and wait</u>
		- a process holding at least one resource is waiting to acquire additional resources held by other processes
	3. <u>No preemption</u>
		- a resource can be released only voluntarily by the process holding it, after that process has completed its task using the resource
	4. <u>Circular wait</u>
		- there exists a set {T<sub>1</sub>, T<sub>2</sub>, ..., T<sub>n</sub>} of waiting processes such that T<sub>1</sub> is waiting for a resource that is held by T<sub>2</sub>, T<sub>2</sub> is waiting for a resource that is held by T<sub>3</sub>, ..., T<sub>n-1</sub> is waiting for a resource that is held by T<sub>n</sub>, and T<sub>n</sub> is waiting for a resource that is held by T<sub>1</sub>

### Resource Allocation Graph
* Aset of vertices V and a set of edges E
* V is partitioned into two types:
	- T = {T<sub>1</sub>, T<sub>2</sub>, ..., T<sub>n</sub>}, the set consisting of all the processes in the system
	- R = {R<sub>1</sub>, R<sub>2</sub>, ..., R<sub>m</sub>}, the set consisting of all resource types in the system
* *Request edge*
	- Directed edge T<sub>i</sub> -> R<sub>j</sub>
* *Assignment edge*
	- Directed edge R<sub>j</sub> -> T<sub>i</sub>
* A resource allocation graph is used to determine “state” of the resource system

### Resource-Allocation Graph Symbols
* Process
<br /><img width="62.5" height="54" src="https://github.com/missystem/cis415review/blob/master/process.png"><br />
* Resource type with 4 instances
<br /><img width="25" height="25" src="https://github.com/missystem/cis415review/blob/master/resourcetype4instance.png"><br />
* T<sub>i</sub> requests instance of R<sub>j</sub>
<br /><img width="62" height="38" src="https://github.com/missystem/cis415review/blob/master/requestinstance.png"><br />
* T<sub>i</sub> is holding an instance of R<sub>j</sub>
<br /><img width="63" height="38" src="https://github.com/missystem/cis415review/blob/master/holdinginstance.png"><br />

### Example of a Resource Allocation Graph
* The sets T, R, and E:
	- T = {T<sub>1</sub>, T<sub>2</sub>, T<sub>3</sub>}
	- R = {R<sub>1</sub>, R<sub>2</sub>, R<sub>3</sub>, R<sub>4</sub>}
	- E = {T<sub>1</sub> → R<sub>1</sub>, T<sub>2</sub> → R<sub>3</sub>, R<sub>1</sub> → T<sub>2</sub>, R<sub>2</sub> → T<sub>2</sub>, R<sub>2</sub> → T<sub>1</sub>, R<sub>3</sub> → T<sub>3</sub>}
	<br /><img width="135" height="195" src="https://github.com/missystem/cis415review/blob/master/RAG.png"><br />
* Resource instances:
	- One instance of resource type R<sub>1</sub>
	- Two instances of resource type R<sub>2</sub>
	- One instance of resource type R<sub>3</sub>
	- Three instances of resource type R<sub>4</sub>
* Thread states:
	- Thread T<sub>1</sub> is holding an instance of resource type R<sub>2</sub> and is waiting for an instance of resource type R<sub>1</sub>
	- Thread T<sub>2</sub> is holding an instance of R<sub>1</sub> and an instance of R<sub>2</sub> and is waiting for an instance of R<sub>3</sub>
	- Thread T<sub>3</sub> is holding an instance of R<sub>3</sub>
* Given the definition of a resource-allocation graph, it can be shown that
	- If the graph contains no cycles, then no thread in the system is deadlocked.
	- If the graph does contain a cycle, then a deadlock may exist

### Resource Allocation Graph With A Deadlock
<br /><img width="140" height="200" src="https://github.com/missystem/cis415review/blob/master/RAGwithDeadlock.png"><br />
* Look for hold and wait conditions that create a circular waiting state
	- Every process is waiting on a resource that is being held by another process in the cycle
* If there is a circular wait state, then no process can proceed because they are all blocked
	- Every process is waiting indefinitely because no resource will be released

### Resource Allocation Graph With a Cycle but No Deadlock.
<br /><img width="165" height="195" src="https://github.com/missystem/cis415review/blob/master/RAGwithCyclenoDeadlock.png"><br />
* It is possible for there to be a cycle in the resource allocation graph, but the resource system is not deadlocked
* There is no deadlock. Observe that thread T<sub>4</sub> may release its instance of resource type R<sub>2</sub> 
* That resource can then be allocated to T<sub>3</sub>, breaking the cycle

### Summary about Resource Allocation Graph
* If resource allocation graph contains no cycles:
	- no deadlock
* If resource allocation graph contains a cycle: 
	- If there is only one instance per resource type <br />
	==> deadlock
	- If there are several instances per resource type <br />
	==> possibility of deadlock

### Methods for Handling Deadlocks
* Basically, we need to ensure that the system will never enter a deadlock state
* This can be done in 2 ways:
	- *Deadlock prevention*
	- *Deadlock avoidance*
* Allow the system to enter a deadlocked state, detect it, and recover
	- Need to guarantee that recovery is possible and valid
* Ignore the problem altogether and pretend that deadlocks never occur in the system
	- Used by most operating systems, including UNIX

### Deadlock Prevention – Requirements
* Idea is to restrain the ways request can be made
* Mutual exclusion
	- Not required for sharable resources (e.g., read-only files) 
	- Must hold for non-sharable resources
* Hold and wait
	- Must guarantee that whenever a process requests a resource, it does not hold any other resources (strict)
	- Require process to request and be allocated ALL of its resources before it begins execution
	- Allow process to request resources only when the process has none allocated to it
	- Low resource utilization and starvation possible
* No preemption
	- If a process that is holding some resources requests another resource that cannot be immediately allocated to it, then all resources currently being held by that process are released
	- Preempted resources are added to the list of resources for which the process is waiting
	- Process will be restarted only when it can regain its old resources, as well as the new ones that it is requesting
* Circular wait
	- Impose a total ordering of all resource types, and require that each process requests resources in an increasing order of enumeration

### Example using Pthread Mutex Locks
* Think of the mutex locks as representing a resource being requested
* Does this code generate a deadlock?
* Why or why not?
* What is wrong?
* Hint: order matters
```
/* Thread one runs in this function */ 
void *do_work_one(void *param) 
{
	pthread_mutex_lock(&first_mutex); pthread_mutex_lock(&second_mutex);
		/** 
		* Do some work 
		*/ 
	pthread_mutex_unlock(&second_mutex);
	pthread_mutex_unlock(&first_mutex);

	pthread_exit(0); 
}

/* Thread two runs in this function */ 
void *do_work_two(void *param) 
{
	pthread_mutex_lock(&second_mutex); pthread_mutex_lock(&first_mutex);
		/** 
		* Do some work 
		*/ 
	pthread_mutex_unlock(&first_mutex);
	pthread_mutex_unlock(&second_mutex);

	pthread_exit(0); 
}
```

### An Account Transaction Example
* Transactions 1 and 2 execute concurrently
	- Transaction 1 transfers $25 from account A to account B 
	- Transaction 2 transfers $50 from account B to account A
* Locks are locked in an order, unlocked in reverse
* Deadlock example with lock ordering
```
void transaction(Account from, Account to, double amount) { 
	mutex lock1, lock2;
	lock1 = get_lock(from);
	lock2 = get_lock(to);

    acquire(lock1);
    	acquire(lock2);

	    	withdraw(from, amount);
	        deposit(to, amount);
    	release(lock2);
   	release(lock1);
}
```

### Deadlock Avoidance
* Requires that the system has some additional *a priori* information available
* Simplest and most useful model requires that each process declare the maximum number of resources of each type that it may need
* The deadlock-avoidance algorithm dynamically examines the resource allocation state to ensure that there can never be a circular-wait condition
* Resource allocation state is defined by the number of available and allocated resources, and the *maximum* possible demands of the processes

### Safe State
* When a process requests an available resource, it must be decided if immediate allocation of the resource to the process leaves the system in a safe state
q System is in safe state if there exists a sequence
<T<sub>1</sub>, T<sub>2</sub>, ..., T<sub>n</sub>> of ALL the processes in the system
such that for each T<sub>i</sub>, the resources that T<sub>i</sub> can still request can be satisfied by currently available resources + resources held by all the T<sub>j</sub>, with j < i
	- If T<sub>i</sub> resource needs are not immediately available, then Pi can wait until all T<sub>j</sub> have finished
	- When T<sub>j</sub> is finished, T<sub>i</sub> can obtain needed resources, execute, return allocated resources, and terminate
	- When T<sub>i</sub> terminates, T<sub>i</sub>+1 can obtain its needed resources

### Basic Facts
* If a system is in safe state <br />
	==> no deadlocks
* If a system is in unsafe state <br />
	==> possibility of deadlock
* Avoidance achieved by ensuring that a system will never enter an unsafe state

### State Relationships
* A resource allocation graph can be in 2 mutually exclusive states: 
	- safe
	- unsafe
* In the unsafe state, a resource allocation graph can be vulnerable to deadlock or in a deadlocked condition
* Safe, unsafe, and deadlocked state spaces
<br /><img width="162" height="162" src="https://github.com/missystem/cis415review/blob/master/safeunsafedeadlockstate.png"><br />

### Deadlock Avoidance Algorithms
* Avoidance algorithms prevent deadlocks from ever happening
	- Never allowing an unsafe state to be entered)
* Approaches depend on assumptions about the resource allocation graph
* Single instance of a resource type
	- Use a resource allocation graph to evaluate
* Multiple instances of a resource type
	- Must run an algorithm on the resource allocation graph
	- Use the Banker’s algorithm

### Resource Allocation Graph Scheme
* Claim edge T<sub>i</sub> -> R<sub>j</sub> indicates that process T<sub>j</sub> may request resource R<sub>j</sub>
	- Represented by a dashed line
* Claim edge converts to a *request edge* when a process requests a resource
* Request edge converted to an *assignment edge* when the resource is allocated to the process
* When a resource is released by a process, assignment edge reconverts to a claim edge
* Resources must be claimed a priori in the system

### Resource Allocation Graph for Deadlock Avoidance
<br /><img width="140" height="140" src="https://github.com/missystem/cis415review/blob/master/RAGfordeadlockavoidance.png"><br />

### An Unsafe State in a Resource Allocation Graph.
<br /><img width="140" height="135" src="https://github.com/missystem/cis415review/blob/master/RAGunsafestate.png"><br />

### Resource Allocation Graph Algorithm
* Suppose that process T<sub>i</sub> requests a resource R<sub>j</sub>
* The request can be granted only if converting the request edge to an assignment edge does not result in the formation of a cycle in the resource allocation graph
* Cycles are evaluated using all types of edges, including claim edges

### Banker’s Algorithm
* Suppose each resource type has multiple instances
* Requirements:
	- Each process must a priori claim maximum use
	- When a process requests a resource it may have to wait
	- When a process gets all its resources it must return them in a finite amount of time
* Banker’s algorithms is a bookkeeping method for tracking and assigning resources

### Data Structures for Banker’s Algorithm
* Let n = number of processes, and <br />
	m = number of resources types
* **Available**: Vector of length m. If ```Available[j] = k```, there are k instances of resource type R<sub>j</sub> available.
* **Max**: n x m matrix. If ```Max[i,j] = k```, then process T<sub>i</sub> may request at most k instances of resource type R<sub>j</sub>
* **Allocation**: n x m matrix. If ```Allocation[i,j] = k``` then T<sub>i</sub> is currently allocated k instances of R<sub>j</sub>
* **Need**: n x m matrix. If Need[i,j] = k, then T<sub>i</sub> may need k more instances of R<sub>j</sub> to complete its task <br />
	**Need [i,j] = Max[i,j] – Allocation [i,j]**

### Safety Algorithm
1. Let Work and Finish be vectors of length m and n, respectively. Initialize:<br />
	Work = Available<br />
	Finish[i] = false for i = 0, 1, ..., n-1<br />
2. Find an i such that both: <br />
	Finish[i] = false <br />
	Need<sub>i</sub> ≤ Work <br />
	If no such i exists, go to step 4 <br />
3. Work = Work + Allocation<sub>i</sub> <br />
	Finish[i] = true <br />
	go to step 2 <br />
4. If Finish[i] == true for all i, then the system is in a safe state

### Resource-Request Algorithm for Process Pi
* Request<sub>i</sub> = request vector for process T<sub>i</sub>
* If Request<sub>i</sub>[j] = k then process Pi wants k instances of resource type R<sub>j</sub>
	1. If Request<sub>i</sub> ≤ Needi go to step 2
	Otherwise, raise error condition, since process has exceeded its maximum claim
	2. If Request<sub>i</sub> ≤ Available, go to step 3
	Otherwise Pi must wait, since resources are not available
	3. Pretend to allocate requested resources to T<sub>i</sub>: <br />
	Available = Available – Request<sub>i</sub>; <br />
	Allocation<sub>i</sub> = Allocation<sub>i</sub> + Request<sub>i</sub>; <br />
	Need<sub>i</sub> = Need<sub>i</sub> – Request<sub>i</sub>;<br />
	If safe => the resources are allocated to T<sub>i</sub><br />
	If unsafe => T<sub>i</sub> must wait, restore old resource allocation

### Banker's Algorithm
* 5 processes T<sub>0</sub> through T<sub>4</sub>
* 3 resource types:
	- A (10 instances), B (5 instances), C (7 instances)
* Snapshot at time T<sub>i</sub>:

| Process | Allocation | Max | Available |
|:-------:|:----------:|:---:|:---------:|
|		  |     A B C    | A B C |    A B C    |
| T<sub>0</sub> | 0 1 0 | 7 5 3 | 3 3 2 |
| T<sub>1</sub> | 2 0 0 | 3 2 2 | 	    |
| T<sub>2</sub> | 3 0 2 | 9 0 2 |	    |
| T<sub>3</sub> | 2 1 1 | 2 2 2 |       |
| T<sub>4</sub> | 0 0 2 | 4 3 3 |       |

### Check for Safety
* Matrix *Need* is defined to be Max – Allocation

| Process | Need |
|:-:|:-:|
| | ABC |
| T<sub>0</sub> | 74 | 3 |
| T<sub>1</sub> | 12 | 2 |
| T<sub>2</sub> | 60 | 0 |
| T<sub>3</sub> | 01 | 1 |
| T<sub>4</sub> | 43 | 1 |

* The system is in a safe state since the sequence < T<sub>1</sub>, T<sub>3</sub>, T<sub>4</sub>, T<sub>2</sub>, T<sub>0</sub>> satisfies safety criteria

### P1 Requests (1,0,2)
* Let’s advance the system with T<sub>1</sub> making request (1,0,2)
* Check that Request ≤ Available (that is, (1,0,2) ≤ (3,3,2) => true)

| Process | Allocation | Max | Available |
|:-------:|:----------:|:---:|:---------:|
|		  |    A B C   | A B C | A B C |
| T<sub>0</sub> | 0 1 0 | 7 5 3 | 2 3 0 |
| T<sub>1</sub> | 2 0 0 | 3 2 2 |	    |
| T<sub>2</sub> | 3 0 2 | 9 0 2 |	    |
| T<sub>3</sub> | 2 1 1 | 2 2 2 |       |
| T<sub>4</sub> | 0 0 2 | 4 3 3 |       |

* Executing safety algorithm shows that sequence < P1, P3, P4, P0, P2> satisfies safety requirement
* Can request for (3,3,0) by P4 be granted? 
* Can request for (0,2,0) by P0 be granted?

### Deadlock Detection
* Allow system to enter deadlock state
* Detection algorithm determines if the resource allocation graph is in a deadlock state
* If it is, a recovery scheme is used to get out of it
	- Assume that it is possible to, in fact, recover
	- It is possible that this might require a rollback to a prior consistent state

### Single Instance of Each Resource Type
* Need to maintain a *wait-for* graph 
	- Nodes are processes
	- Pi -> Pj if Pi is waiting for Pj
* Periodically invoke an algorithm that searches for a cycle in the wait-for graph
	- If there is a cycle, there exists a deadlock
* An algorithm to detect a cycle in a graph requires an order of n<sup>2</sup> operations, where n is the number of vertices in the graph

### Resource-Allocation and Wait-For Graph
* (a) Resource-allocation graph. (b) Corresponding wait-for graph.
<br /><img width="390" height="250" src="https://github.com/missystem/cis415review/blob/master/ResourceAllocationandWaitFor.png"><br />

### Several Instances of a Resource Type
* *Available*:  A vector of length m indicates the number of available resources of each type
* *Allocation*: An n x m matrix defines the number of resources of each type currently allocated to each process
* *Request*: An n x m matrix indicates the current request of each process
	- If Request<sub>i</sub>[j] = k, then process T<sub>i</sub> is requesting k more instances of resource type R<sub>j</sub>

### Detection Algorithm
1. Let Work and Finish be vectors of length m and n, respectively Initialize: <br />
	(a) Work = Available <br />
	(b) For i = 1,2, ..., n, if Allocation<sub>i</sub> ≠ 0, then <br />
	Finish[i] = false; otherwise, Finish[i] = true <br />
2. Find an index i such that both:  <br />
	(a) Finish[i] == false <br />
	(b) Request<sub>i</sub> ≤ Work <br />
	If no such i exists, go to step 4 <br />
3. Work = Work + Allocation<sub>i</sub> <br />
	Finish[i] = true <br />
	goto step 2 <br />
4. If Finish[i] == false, for some i, 1 ≤ i ≤ n, <br />
	then the system is in deadlock state
	If Finish[i] == false, then T<sub>i</sub> is deadlocked

### Example of Detection Algorithm
* Five processes P0 through P4
* Three resource types
	- A (7 instances), B (2 instances), and C (6 instances)
* Snapshot at time T0:

| Process | Allocation | Max | Available |
|:-------:|:----------:|:---:|:---------:|
|		  |    A B C   | A B C | A B C |
| T<sub>0</sub> | 0 1 0 | 0 0 0 | 0 0 0 |
| T<sub>1</sub> | 2 0 0 | 2 0 2 |	    |
| T<sub>2</sub> | 3 0 3 | 0 0 0 |	    |
| T<sub>3</sub> | 2 1 1 | 1 0 0 |       |
| T<sub>4</sub> | 0 0 2 | 0 0 2 |       |

* Sequence <T<sub>0</sub>, T<sub>2</sub>, T<sub>3</sub>, T<sub>1</sub>, T<sub>4</sub>> gives Finish[i] = true for all i

### Deadlock?
* T<sub>2</sub> requests an additional instance of type C

| Process | Request | 
|:-------:|:-------:|
|		  |  A B C  |
| T<sub>0</sub> | 0 0 0 |
| T<sub>1</sub> | 2 0 2 |
| T<sub>2</sub> | 0 0 1 |
| T<sub>3</sub> | 1 0 0 |
| T<sub>4</sub> | 0 0 2 |

* State of system?
	- Can reclaim resources held by process P<sub>0</sub>, but insufficient resources to fulfill other processes requests
❍ Deadlock exists, consisting of processes T<sub>1</sub>, T<sub>2</sub>, T<sub>3</sub>, and T<sub>4</sub>

### Detection Algorithm Usage
* When, and how often, to invoke depends on:
	- How often a deadlock is likely to occur?
	- How many processes will need to be rolled back?
		- one for each disjoint cycle
* If detection algorithm is invoked arbitrarily, there may be many cycles in the resource graph and so we would not be able to tell which of the many deadlocked processes “caused” the deadlock

### Deadlock Recovery – Process and Thread Termination
* Aborting
	- Abort all deadlocked processes
	- Abort one process at a time until the deadlock cycle is eliminated
* Different schemes
	- Abort all deadlocked processes
	- Abort one process at a time until the deadlock cycle is eliminated
* In which order should we choose to abort?
	- Priority of the process
	- How long process has computed, and how much longer to completion
	- Resourcestheprocesshasused
	- Resources process needs to complete
	- How many processes will need to be terminated
	- Is process interactive or batch?

### Deadlock Recovery – Resource Preemption
1. Selecting a victim
	- Attempt to minimize cost
2. Rollback
	- Return to some safe state
	- Restart process for that state
3. Starvation
	- Same process may always be picked as victim, include number of rollback in cost factor

---

## Chapter 8 Summary from OSC
*  **Deadlock occurs** in a set of processes when every process in the set is waiting for an event that can only be caused by another process in the set.
* **4 necessary conditions** for deadlock (Deadlock is only possible when all four conditions are present)
	1. mutual exclusion
	2. hold and wait
	3. no preemption
	4. circular wait
* Deadlocks can be **modeled** with **Resource-Allocation Graphs**
	- where a cycle indicates deadlock
* Deadlocks can be **prevented** by ensuring that one of the 4 necessary conditions for deadlock cannot occur
	- Of the four necessary conditions, eliminating the circular wait is the only practical approach.
* Deadlock can be **avoided** by using the **Banker’s Algorithm**
	- which does not grant resources if doing so would lead the system into an unsafe state where deadlock would be possible
* A **deadlock-detection algorithm**
	- can evaluate processes and resources on a running system to determine if a set of processes is in a deadlocked state
* If deadlock does occur, a system can attempt to **recover** from the deadlock by 
	- aborting one of the processes in the circular wait
	- preempting resources that have been assigned to a deadlocked process

